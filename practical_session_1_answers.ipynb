{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/SimonKoop/jax_tutorial_trial/blob/main/practical_session_1_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# JAX Tutorial\n",
    "## 1. What's JAX?\n",
    "* Accellerated Array computations (like Numpy but on GPU)\n",
    "* JIT (just-in-time) compiled to XLA (Accellerated Linear Algebra)\n",
    "* Autograd and other transformations\n",
    "\n",
    "\n",
    "jax.numpy: stand-in replacement for numpy\n",
    "\n",
    "NB some caveats apply! More on that later, but also, see [this page](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) for a more comprehensive guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should install all requirements if you're running this in colab\n",
    "#%pip install git+https://github.com/SimonKoop/common_jax_utils\n",
    "# You might get an error for some typeguard version not being correct, you can safely ignore this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 19:29:27.620572: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.20). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26894143 0.5        0.73105854 0.8807971 ]\n",
      "[0.26894143 0.5        0.73105854 0.8807971 ]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "def sigmoid(x:jax.Array)->jax.Array:  # many commonly used activation functions can be found in jax.nn (e.g. jax.nn.sigmoid)\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "some_vector = jnp.array([-1., 0., 1., 2.])\n",
    "print(sigmoid(some_vector))\n",
    "\n",
    "# we can JIT compile the functions\n",
    "sigmoid_jitted = jax.jit(sigmoid)\n",
    "print(sigmoid_jitted(some_vector))  # first time calling this will take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 μs ± 9.83 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sigmoid(some_vector).block_until_ready()  # if you want to know why we use block_until_ready(), read https://jax.readthedocs.io/en/latest/async_dispatch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.5 μs ± 1.26 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sigmoid_jitted(some_vector).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Use the tools from jax.numpy to compute the following:\n",
    "$$\n",
    "\\mathrm{sigmoid}\\left(\\left(\\begin{matrix}1. & 0.5 & 0.25\\\\ 0. & 2. & 0.25 \\\\ 0. & 0. & 1.\\end{matrix}\\right)\\left(\\begin{matrix}1. \\\\-1. \\\\ .3\\end{matrix}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.6399161 , 0.12730505, 0.5744425 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put your answer here\n",
    "matrix = jnp.array([[1., 0.5, 0.25],[0., 2., 0.25], [0., 0., 1.]], dtype=jnp.float32)\n",
    "vector = jnp.array([1., -1., .3], dtype=jnp.float32)\n",
    "sigmoid(matrix@vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. First quirk: pure functions\n",
    "In order to make a.o. JIT compilation and automatic differentiation easier, the creators of JAX opted for a more functional style of programming. For many of the useful transforms of JAX to work, you need to use **pure functions** i.e. functions without side-effects.\n",
    "\n",
    "Because of this, **jax arrays are immutable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_numpy_array=array([[0. , 0. , 0. ],\n",
      "       [0. , 0.5, 1. ],\n",
      "       [0. , 0. , 0. ]])\n"
     ]
    }
   ],
   "source": [
    "# jax arrays vs numpy arrays example\n",
    "import numpy as np\n",
    "\n",
    "some_numpy_array = np.zeros((3, 3))\n",
    "some_numpy_array[1] = np.linspace(0., 1., 3)\n",
    "print(f\"{some_numpy_array=}\")\n",
    "\n",
    "some_jax_array = jnp.zeros((3, 3))\n",
    "# some_jax_array[1] = jnp.linspace(0., 1., 3)  # this will give an error, uncomment if you want to try\n",
    "# print(f\"{some_jax_array=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_jax_array=Array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)\n",
      "some_new_jax_array=Array([[0. , 0. , 0. ],\n",
      "       [0. , 0.5, 1. ],\n",
      "       [0. , 0. , 0. ]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# how to do this instead\n",
    "some_new_jax_array = some_jax_array.at[1].set(jnp.linspace(0., 1., 3))\n",
    "print(f\"{some_jax_array=}\")  # also, note that the default dtype is jnp.float32. \n",
    "print(f\"{some_new_jax_array=}\")  # To be able to use float64, you'd have to enable it at startup, but maybe just don't unless you really need to.\n",
    "\n",
    "# Note that the original array remains unmodified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, if we want to use function transformations like `jax.jit` or `jax.grad` (more on this later), we will have to use pure functions. To gain some understanding of what this means, let's look at something that is *not* a pure function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_layer(some_x1)=Array([1., 0., 0.], dtype=float32)\n",
      "my_layer(some_x2)=Array([ 2. , -0.9,  1.8], dtype=float32)\n",
      "my_layer(some_x1)=Array([ 1.3       , -0.17999999,  0.35999998], dtype=float32)\n",
      "my_layer_jitted(some_x1)=Array([1., 0., 0.], dtype=float32)\n",
      "my_layer_jitted(some_x2)=Array([ 2., -1.,  2.], dtype=float32)\n",
      "my_layer_jitted(some_x1)=Array([1., 0., 0.], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# the following works absolutely fine untill we try to JIT things (or use autograd, or any of the other useful function transformations)\n",
    "class SelfChangingLayer:\n",
    "\n",
    "    def __init__(self, initial_weights:jax.Array):\n",
    "        \"\"\" \n",
    "        Initialize the SelfChangingLayer\n",
    "        :parameter initial_weights: weights matrix, is.e. jax.Array with shape (N, N)\n",
    "        \"\"\"\n",
    "        self.weights = initial_weights\n",
    "\n",
    "    def __call__(self, x:jax.Array):\n",
    "        \"\"\" \n",
    "        forward pass\n",
    "        :parameter x: vector i.e. jax.Array with shape (N,)\n",
    "        \"\"\"\n",
    "        output = self.weights@x\n",
    "        # and now for the side-effect:\n",
    "        self.weights = .9*self.weights + .1*(output[:, None]@output[None, :])  # output[:, None] has shape (N, 1) and output[None, :] has shape (1, N)\n",
    "        return output\n",
    "    \n",
    "my_layer = SelfChangingLayer(jnp.eye(3))\n",
    "some_x1 = jnp.array([1., 0., 0.])\n",
    "some_x2 = jnp.array([2., -1., 2.])\n",
    "\n",
    "print(f\"{my_layer(some_x1)=}\")\n",
    "print(f\"{my_layer(some_x2)=}\")\n",
    "print(f\"{my_layer(some_x1)=}\")\n",
    "\n",
    "# As you can see, my_layer has side-effects. Due to this, the same input results in two different outputs.\n",
    "# That means we can't jit compile it, or use automatic differentiation with it, without getting wrong answers\n",
    "\n",
    "my_layer_jitted = jax.jit(SelfChangingLayer(jnp.eye(3)))\n",
    "print(f\"{my_layer_jitted(some_x1)=}\")\n",
    "print(f\"{my_layer_jitted(some_x2)=}\")\n",
    "print(f\"{my_layer_jitted(some_x1)=}\")  # as you can see, these outcomes are not correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "The way we typically deal with this, is by making the state an argument to the function and returning an updated state as an output of the function. Write a pure function analogue of the `SelfChangingLayer` above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_1=Array([1., 0., 0.], dtype=float32)\n",
      "output_2=Array([ 2. , -0.9,  1.8], dtype=float32)\n",
      "output_3=Array([ 1.3       , -0.17999999,  0.35999998], dtype=float32)\n",
      "output_1=Array([1., 0., 0.], dtype=float32)\n",
      "output_2=Array([ 2. , -0.9,  1.8], dtype=float32)\n",
      "output_3=Array([ 1.3       , -0.17999999,  0.35999998], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def self_changing_layer(x:jax.Array, weights:jax.Array) -> tuple[jax.Array, jax.Array]:\n",
    "    output = weights @ x \n",
    "    weights = .9*weights +.1*(output[:, None]@output[None, :])\n",
    "    return output, weights\n",
    "\n",
    "# uncomment the following to test your code\n",
    "\n",
    "weights = jnp.eye(3)\n",
    "output_1, weights = self_changing_layer(some_x1, weights)\n",
    "output_2, weights = self_changing_layer(some_x2, weights)\n",
    "output_3, weights = self_changing_layer(some_x1, weights)\n",
    "print(f\"{output_1=}\\n{output_2=}\\n{output_3=}\")\n",
    "\n",
    "self_changing_layer_jitted = jax.jit(self_changing_layer)\n",
    "weights = jnp.eye(3)\n",
    "output_1, weights = self_changing_layer_jitted(some_x1, weights)\n",
    "output_2, weights = self_changing_layer_jitted(some_x2, weights)\n",
    "output_3, weights = self_changing_layer_jitted(some_x1, weights)\n",
    "print(f\"{output_1=}\\n{output_2=}\\n{output_3=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this mean we can't use object-oriented programming with JAX? No, later in this tutorial we'll look at a package called Equinox that will help us with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Higher order functions\n",
    "Another aspect of functional programming that JAX makes heavy use of, is higher order functions. That means: functions that either take another function as an argument, or return another function as their output (or both). One example we have already seen of a higher order function is `jax.jit`: it takes a (pure) function as its input, and returns a jit-compiled function as its output. We also alluded to the the existence of `jax.grad`, let's now look at what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.: 0.25, 1.:0.1966119408607483, -1.:0.1966119408607483\n",
      "0.: 0.25, 1.:0.19661195576190948, -1:0.1966119408607483\n"
     ]
    }
   ],
   "source": [
    "sigmoid_grad = jax.grad(sigmoid)  # this gives the gradient of sigmoid with respect to its first (and only) argument. \n",
    "def sigmoid_grad_manual(x):\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "print(f\"0.: {sigmoid_grad(0.)}, 1.:{sigmoid_grad(1.)}, -1.:{sigmoid_grad(-1.)}\")\n",
    "print(f\"0.: {sigmoid_grad_manual(0.)}, 1.:{sigmoid_grad_manual(1.)}, -1:{sigmoid_grad_manual(-1.)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One salient detail of `jax.grad` is that the function of which we compute the gradient must have scalar output. \n",
    "When we feed `sigmoid` a vector, it returns a vector, so the following gives `TypeError: Gradient only defined for scalar-output functions. Output had shape: (4,).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid_grad(some_vector) # uncomment if you want to try "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use `sigmoid_grad` to compute the gradient of the *scalar* function sigmoid on a batch (vector) of scalars, we can use another function transformation: `jax.vmap`.\n",
    "\n",
    "What `jax.vmap` does, is it takes a function f, and it returns a version of f that works on a batch of inputs (this is a vectorized function, so the computations happen in parallel). Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax.vmap(sigmoid_grad)(some_vector)=Array([0.19661194, 0.25      , 0.19661194, 0.10499357], dtype=float32)\n",
      "sigmoid_grad_manual(some_vector)=Array([0.19661194, 0.25      , 0.19661196, 0.10499357], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{jax.vmap(sigmoid_grad)(some_vector)=}\")\n",
    "print(f\"{sigmoid_grad_manual(some_vector)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for many functions (such as sigmoid and the derivative of a sigmoid), it is easy to write a function that automatically gets vectorized when applied to higher-dimensional input. But when writing more complicated functions, `jax.vmap` allows us to write the function for inputs for which we easily understand what it should look like, and then automatically apply it correctly to higher dimensional inputs without worrying about this. \n",
    "\n",
    "### Exercise 3\n",
    "Use the documentation of [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) to create the following function:\n",
    "$$\n",
    "\\mathrm{target\\_function\\_1}(\\mathrm{value}, \\mathrm{weights}) = \\left(\\left(\\nabla_x \\mathrm{self\\_changing\\_layer\\_square}\\right)(\\mathrm{value}, \\mathrm{weights})[0],\\  \\mathrm{self\\_changing\\_layer}(\\mathrm{value},\\, \\mathrm{weights}) [1]\\right)\n",
    "$$\n",
    "where $f()[i]$ is the element at index $i$ of the tuple resulting from $f()$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 3.9295735, -0.9079778,  1.8159556], dtype=float32),\n",
       " Array([[ 1.384392  , -0.19168504,  0.38337007],\n",
       "        [-0.19168504,  0.7280632 , -0.1439266 ],\n",
       "        [ 0.38337007, -0.1439266 ,  0.94395316]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def self_changing_layer_square(x, weights):\n",
    "    out, new_weights = self_changing_layer(x, weights)\n",
    "    return jnp.square(out).sum(), new_weights\n",
    "\n",
    "target_function = jax.grad(self_changing_layer_square, has_aux=True)\n",
    "target_function(some_x1, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(target_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.384392  , -0.19168504,  0.38337007],\n",
       "       [-0.19168504,  0.7280632 , -0.1439266 ],\n",
       "       [ 0.38337007, -0.1439266 ,  0.94395316]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the resulting weights are correct:\n",
    "self_changing_layer(some_x1, weights)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "In the following code block, you are given one array containing 20 different $3$-vectors, and an array containing 30 different $3$-vectors. Use `vmap` (twice) with the given outer product function to create a $20\\times 30\\times 3\\times 3$ array with in location $(i, j)$ a $3\\times 3$-matrix containing the result of taking the outer product of the $i^{\\text{th}}$ vector in the first array with the $j^{\\text{th}}$ vector in the second array. \n",
    "\n",
    "Hint: if you want vmap to not vmap over any dimension in some array, you can provide `None` for that array in `in_axes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 30, 3, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_1 = jnp.linspace(0., 1., 60).reshape((20, 3))\n",
    "vectors_2 = jnp.linspace(0., 1., 90).reshape((30, 3))\n",
    "\n",
    "def outer_product(u: jax.Array, v: jax.Array)->jax.Array:\n",
    "    \"\"\" \n",
    "    Vector outer product\n",
    "    :parameter u: jax.Array of shape (n,)\n",
    "    :parameter v: jax.Array of shape (n,)\n",
    "    :return: jax.Array of shape (n, n) representing the outer product u@v^T\n",
    "    \"\"\"\n",
    "    return u[:, None]@v[None, :]\n",
    "\n",
    "\n",
    "# your code goes here\n",
    "jax.vmap(jax.vmap(outer_product, (None, 0)), (0, None))(vectors_1, vectors_2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random numbers\n",
    "When we use 'random' numbers in machine learning, they typically aren't truely random. Instead, we are using a [pseudo random number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) (**PRNGs**) to create (deterministic) sequences of outputs that the right statistical properties. These PRNGs typically have a *seed*, which determines what sequence is going to be generated, and some internal state. In e.g. numpy and pytorch, you can set this seed, and the (underlying) PRNG that is used then automatically updates its state every time you draw a random number.\n",
    "\n",
    "Now, as we have discussed above, JAX doesn't play too nicely with global states. Instead, we have to keep track of the PRNG state, typically refered to as a **key**, ourselves. If we pass the same key to the same random function twice, we get the same result twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array((), dtype=key<fry>) overlaying:\n",
      "[  0 123]\n",
      "\n",
      "random.normal(key, shape=(3,))=Array([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)\n",
      "random.normal(key, shape=(3,))=Array([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)\n",
      "\n",
      "Array((), dtype=key<fry>) overlaying:\n",
      "[  0 123]\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "key = random.key(123)\n",
    "print(key)\n",
    "\n",
    "print(f\"\\n{random.normal(key, shape=(3,))=}\")\n",
    "print(f\"{random.normal(key, shape=(3,))=}\\n\")\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get new pseudo random numbers, we need to split the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array((), dtype=key<fry>) overlaying:\n",
      "[1896456402   17229315]\n",
      "Array((), dtype=key<fry>) overlaying:\n",
      "[4081828428 1707601653]\n",
      "\n",
      "random.normal(subkey, shape=(3,))=Array([-0.56996626, -0.6440589 ,  0.28660855], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "key, subkey = random.split(key)\n",
    "print(key)\n",
    "print(subkey)\n",
    "print(f\"\\n{random.normal(subkey, shape=(3,))=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing this, **make sure not to re-use used keys!** A good habit is to only ever use subkey to pass to functions, and use key to keep track of your random state. So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1124845\n",
      "-0.70996124\n"
     ]
    }
   ],
   "source": [
    "key, subkey = random.split(key)\n",
    "print(random.normal(subkey))\n",
    "key, subkey = random.split(key)\n",
    "print(random.normal(subkey))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that key and subkey are just `jax.Array`s, so we can e.g. `vmap` over subkey if need be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.0297086 ,  2.3845856 , -1.8143567 , -0.80407333, -0.8641458 ],      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_complicated_random_function(key):\n",
    "    # imagine that this is some complicated code that is hard to manually write in a vectorized way\n",
    "    return random.normal(key)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "subkey_array = random.split(subkey, 5)  # if we wanted say a grid instead, we could instead do random.split(subkey, (5, 5)) where (5, 5) would be the shape of the grid of subkeys\n",
    "jax.vmap(my_complicated_random_function)(subkey_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "Look at the documentation of [`jax.lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan) and [`jax.lax.select`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html#jax.lax.select) to write a function that performs the following random walk by scanning over an array of prng keys:\n",
    "\n",
    "$$\n",
    "x^0 = (0, 0)\\\\\n",
    "x^i = \\begin{cases}x^{i-1}+\\epsilon^i&\\text{with probability }\\frac{1}{2}\\\\(x^{i-1}_1, x^{i-1}_0) + \\epsilon^i&\\text{with probability }\\frac{1}{2}\\end{cases}\\\\\n",
    "\\text{where}\\\\\n",
    "\\epsilon^i \\sim \\mathcal{N}(\\underline{0}, I)\n",
    "$$\n",
    "\n",
    "Then use [`jax.vmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html) to simulate 10 random walks of length 20 in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "def random_flip(x:jax.Array, key:jax.Array)->jax.Array:\n",
    "    \"\"\" \n",
    "    Flips the entries in x randomly with probability 1/2\n",
    "    :parameter x: (2,)-array of floats\n",
    "    :parameter key: prng key for deciding whether to flip or not\n",
    "    :return: array of same shape and type as x\n",
    "    \"\"\"\n",
    "    random_float = random.uniform(key)\n",
    "    return jax.lax.select(random_float<.5, x, x[::-1])\n",
    "\n",
    "def step_function(x_old:jax.Array, key:jax.Array)->tuple[jax.Array, jax.Array]:\n",
    "    \"\"\" \n",
    "    Step function for the jax.lax.scan that simulates the random walk\n",
    "    :parameter x_old:  (2,)-array of floats representing x^{i-1}\n",
    "    :parameter key: prng key for all the randomness in this step\n",
    "    :return: (x_new, x_new)  where x_new has the same shape as x_old and represents x^i\n",
    "    \"\"\"  # why do we want to return x_new twice?\n",
    "    key_1, key_2 = random.split(key)  # Don't forget to split the keys!\n",
    "    epsilon = random.normal(key_2, (2,))\n",
    "    x_new = random_flip(x_old, key_1) + epsilon\n",
    "    return x_new, x_new\n",
    "\n",
    "def simulate_random_walk(key:jax.Array, length:int)->jax.Array:\n",
    "    \"\"\" \n",
    "    Simulate one random walk.\n",
    "    :parameter key: prng key for all randomness in this random walk\n",
    "    :parameter length: length of the random walk\n",
    "    :return: (length, 2)-array of floats representing the random walk\n",
    "    \"\"\"\n",
    "    keys = random.split(key, length-1) # split the key\n",
    "    x_0 = jnp.zeros((2,))\n",
    "    _, random_walk = jax.lax.scan(\n",
    "        step_function, \n",
    "        x_0, \n",
    "        keys\n",
    "    )\n",
    "    return jnp.concatenate([x_0[None, :], random_walk])\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "subkeys = random.split(subkey, 10)\n",
    "result = jax.vmap(simulate_random_walk, in_axes=(0, None))(subkeys, 20)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side-note, although there are good reasons for JAX to take this approach to PRNGs, it can be a bit cumbersome to do this by hand all the time, especially when you're e.g. writing neural network architectures, where each layer will need a prng key for initialization and you might not know by heart beforehand how many keys you will need. I've written a package with convenience utilities called `common_jax_utils` that includes a generator `key_generator` that you can use in such situations.\n",
    "\n",
    "I would recommend against using it in any code that needs to be jitted or vmapped, although, as long as you write all of your functions in a way that they do not have side-effects (**so don't write functions that expect a key_generator as an argument**), you should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random.normal(next(key_gen))=Array(1.1852474, dtype=float32)\n",
      "random.normal(next(key_gen))=Array(0.54125774, dtype=float32)\n",
      "random.normal(next(key_gen))=Array(-0.83737814, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import common_jax_utils as cju\n",
    "key, subkey = random.split(key)\n",
    "key_gen = cju.key_generator(subkey)\n",
    "\n",
    "print(f\"{random.normal(next(key_gen))=}\")\n",
    "print(f\"{random.normal(next(key_gen))=}\")\n",
    "print(f\"{random.normal(next(key_gen))=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Control flow and function transformations\n",
    "### 5.1 Jitting / vmapping\n",
    "Sometimes, we need to use control flow, such as `if`-`else` statements or `for` loops. This doesn't always play nice with `jax.jit` and `jax.vmap`. In some of the above exercises, we already saw some `jax.lax` control flow alternatives such as `jax.lax.select` and `jax.lax.scan`. The question then remains: when should we use these control flow functions from `jax.lax`, and when is it okay to just use python control statements?\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow provides a good, elaborate explanation of this, and you should read this. Nonetheless, I'd like to provide a quick rule of thumb with a rationale behind it here.\n",
    "\n",
    "Basically, `jax.jit` keeps track of the shape and dtype of arguments to a jitted function. When a combination of argument shapes/dtypes is encountered that hasn't been encountered before, the function gets compiled *for this specific combination* and the compiled code gets stored, so that when this combination is encountered again, the compiled code can be used again and no re-compilation is necessary. If you play around with this, you'll find that the first time you use a jitted function on a specific array shape, it takes *much* longer than each next time you use it on the same shape.\n",
    "\n",
    "This brings us to the **first rule of thumb** when it comes to control flow and jitting functions: *when the control flow depends **only on the shapes/dtypes** of arguments, you can safely use python control flow statements*. Some examples are:\n",
    "`if len(input_array.shape) == 3:...` or `if input_array.shape[3] % 2 == 0:...` or `if input_array.dtype == jnp.int16:...`.\n",
    "In principle the same holds for e.g. loops, but *if you use long loops (i.e. with many iterations) within a jitted function, the compilation step will likely take very long!*\n",
    "\n",
    "The first step towards jit-compiling a function is called tracing. The function is run on *tracer* objects in order to record the sequence of operations specified by the function. These tracer objects encode the shape and dtype of what they are representing, **but not the actual values**. That means that if you use regular Python control flow (like `if` and `else`, or `while`) based on the values, that you will either get errors, or wrong results. \n",
    "\n",
    "This brings us to the **second rule of thumb** when it comes to control flow and jitting functions: *when the control flow depends on the **values** of arguments, you should use [control flow primitives](https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators).* For example `jax.lax.select(r<.5, x_i, x_i[::-1])`. \n",
    "\n",
    "\n",
    "Another useful resource on this, that you should definitely read, is https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n",
    "\n",
    "\n",
    "As a final **TL;DR on jitting and control flow**:\n",
    "* values depending on values **requires control flow operators**\n",
    "* values depending on shapes is fine (you can use python control flow statements)\n",
    "* shapes depending on shapes is fine, (as long as you don't use jax to make intermediate computations, so `jnp.zeros((np.prod(x.shape),))` is fine but `jnp.zeros((jnp.prod(x.shape)))` wil cause problems when jitted)\n",
    "* shapes depending on values is **not fine** (although in some cases you can get around this by indicating to `jax.jit` that some arguments should not be traced by marking them as *static*).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running f():\n",
      "  x = Traced<ShapedArray(float32[3,4])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  y = Traced<ShapedArray(float32[4])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  result = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([9.449684 , 6.446216 , 7.7138443], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example making tracing explicit\n",
    "@jax.jit  # decorator syntax, see e.g https://pythonbasics.org/decorators/\n",
    "def f(x, y):\n",
    "    print(\"Running f():\")\n",
    "    print(f\"  x = {x}\")\n",
    "    print(f\"  y = {y}\")\n",
    "    result = jnp.dot(x + 1, y + 1)\n",
    "    print(f\"  result = {result}\")\n",
    "    return result\n",
    "\n",
    "x = np.random.randn(3, 4)\n",
    "y = np.random.randn(4)\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side-note, if you want to print values for e.g. debugging purposes, just using the Python `print` function will not give you the desired result when working with jitted functions. Read the material in https://jax.readthedocs.io/en/latest/debugging/index.html and in https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html for more on the topic of **debugging transformed functions**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Complete the following python functions in a way that they can be correctly jit-compiled and then test the jit-compiled versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 4. 8.]\n",
      "[[0. 3.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def f1(x:jax.Array):\n",
    "    \"\"\" \n",
    "    :parameter x: jax.Array (of any shape)\n",
    "    :return: an array with elements y_i = x_i**2 if x_i <0 else x_i **3\n",
    "    \"\"\"\n",
    "    return jnp.where(x<0, x**2, x**3)\n",
    "\n",
    "def f2(a: jax.Array, b: jax.Array):\n",
    "    \"\"\" \n",
    "    :parameter a: jax.Array of shape (n_0, ..., n_k)\n",
    "    :parameter b: jax.Array of shape (m_0, ..., m_k)\n",
    "    :return: a jax.Array c of shape (max(n_0, m_0), ..., max(n_k, m_k))\n",
    "        with c[i_0..., i_k] = aa[i_0, ..., i_k] + bb[i_0, ..., i_k]\n",
    "        where aa[i_0, ..., i_k] = a[i_0, ..., i_k] if i_0<n_0, ..., i_k < n_k else max(n_0, ..., n_k)\n",
    "        and b[i_0, ..., i_k] = b[i_0, ..., i_k] if i_0<m_0, ..., i_k < m_k else max(m_0, ..., m_k)\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    size = [max(n, m) for n, m in zip(a.shape, b.shape)]\n",
    "\n",
    "    aa = jnp.pad(a, pad_width=[(0, s-n) for s, n in zip(size, a.shape)], mode='constant', constant_values=max(a.shape))\n",
    "    bb = jnp.pad(b, pad_width=[(0, s-m) for s, m in zip(size, b.shape)], mode='constant', constant_values=max(b.shape))\n",
    "    \n",
    "    return aa+bb\n",
    "\n",
    "print(f1(jnp.array([-1., 1., -2., 2.])))\n",
    "print(f2(jnp.array([[0., 1.]]), jnp.array([[0.], [1.]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Control flow and grad\n",
    "Mostly there is no problem here (until we want to also `jit` the `grad` of a function). When using `lax.while_loop` or `lax.fori_loop`, automatic differentiation is restricted to [forward mode differentiation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobian-vector-products-jvps-aka-forward-mode-autodiff) (see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#summary). But you're unlikely to run into this during this course.\n",
    "\n",
    "One salient side-note about control-flow and automatic differentiation, is that if you have a function that is only partly defined, such as `jnp.log`, and you want to make it safe using `jnp.where`, you should put the `jnp.where` **inside** the `jnp.log` so as to prevent *NaN*s from occuring: see https://jax.readthedocs.io/en/latest/faq.html#gradients-contain-nan-where-using-where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pytrees and related utilities\n",
    "Before we really get into defining and training Neural Networks in JAX, there is one more very important concept in JAX: pytrees. Before we get to what they really are, you can informally think of pytrees as any container (of containers of...) of `jax.Array`s. This really isn't quite accurate, but for now it's good enough.\n",
    "\n",
    "Why are these relevant? Well, remember that because our functions have to be pure, any state that we might want to have needs to be passed around between them? That state will often not be just at single array. Think of e.g. all the weights and biases of a Neural Network, and think of the momentum for all those weights and biases that some optimizers need to keep track of. For an MLP it might be reasonable to just put all of them into a tuple, but when we work with more complicated networks, it might be helpful for the structure in which we store all of these weights and biases to resemble the architecture of the network.\n",
    "\n",
    "Fortunately, many jax functions, such as `jax.jit`, `jax.grad`, `jax.vmap`, and `jax.lax.scan` play really nicely with these pytrees. And for functions that don't, there are utilities such as `jax.tree.map` and `jax.tree_util.tree_map_with_path`. Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary(jax.vmap(some_function, in_axes=(0, None))(a, b))='array(10, 2)'\n",
      "summary(jax.vmap(some_function, in_axes=(0, (None, 1)))(a, b_alt))='array(10, 2)'\n",
      "summary(jax.value_and_grad(another_function, argnums=1)(a, b))=\"('array()', ['array(2, 3)', 'array(2,)'])\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"('array(10,)', ('array(10, 2, 3)', 'array(10, 2)'))\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from common_jax_utils.debug_utils import summary\n",
    "\n",
    "def some_function(a:jax.Array, b:tuple[jax.Array, jax.Array]):\n",
    "    \"\"\" \n",
    "    :parameter a: array of shape (in_channels,)\n",
    "    :parameter b: tuple of two arrays\n",
    "        first array of shape (out_channels, in_channels)\n",
    "        second array of shape (out_channels,)\n",
    "    :return: array of shape (out_channels,)\n",
    "    \"\"\"\n",
    "    return (b[0]@a)/(1+jnp.abs(b[1]))\n",
    "\n",
    "a = random.normal(next(key_gen), (10, 3))\n",
    "b = [\n",
    "    random.normal(next(key_gen), (2, 3)),\n",
    "    random.normal(next(key_gen), (2,))\n",
    "]\n",
    "\n",
    "print(f\"{summary(jax.vmap(some_function, in_axes=(0, None))(a, b))=}\") # vmap over the 0-axis of a and over none of the axes in any element in b\n",
    "\n",
    "b_alt = (\n",
    "    random.normal(next(key_gen), (2, 3)),\n",
    "    random.normal(next(key_gen), (2, 10))\n",
    ")\n",
    "\n",
    "print(f\"{summary(jax.vmap(some_function, in_axes=(0, (None, 1)))(a, b_alt))=}\") # vmap over the 0-axis of a and over none of the axes in the first element of b_alt and over the 1-axis of the second element of b_alt\n",
    "\n",
    "def another_function(a:jax.Array, b:tuple[jax.Array, jax.Array]):\n",
    "    \"\"\" \n",
    "    :parameter a: array of shape (batch, in_channels)\n",
    "    :parameter b: tuple of two arrays\n",
    "        first array of shape (out_channels, in_channels)\n",
    "        second array of shape (out_channels,)\n",
    "    :return: array of shape (out_channels,)\n",
    "    \"\"\"\n",
    "    return jnp.mean(\n",
    "        jnp.square(\n",
    "            jax.vmap(some_function, in_axes=(0, None))(a, b)\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"{summary(jax.value_and_grad(another_function, argnums=1)(a, b))=}\")  # value and gradient w.r.t b, where gradient has the same shape as b, including the same pytree structure\n",
    "summary(\n",
    "    jax.vmap(jax.value_and_grad(another_function, argnums=1), in_axes=(None, (None, 1)))(a, b_alt)  # can you explain why every array in the result has 10 for its 0-axis?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Complete the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cju.debug_utils.summary(jax.vmap(simple_mlp, in_axes=(0, None, None))(batch, weights, biases))='array(10, 1)'\n",
      "cju.debug_utils.summary(simple_train_step((weights, biases), (batch, ys)))=\"((('array(16, 3)', 'array(16, 16)', 'array(1, 16)'), ('array(16,)', 'array(16,)', 'array(1,)')), 'array()')\"\n",
      "(('array(16, 3)', 'array(16, 16)', 'array(1, 16)'), ('array(16,)', 'array(16,)', 'array(1,)'))\n",
      "[303.88705   837.9702     43.880123    5.9955125   4.6102185   3.1268833\n",
      "   2.637646    2.569746    2.2857032   1.6449605   1.8573601   1.9932003\n",
      "   1.6272165   1.5321568   1.71918     1.4262049   1.386418    1.213356\n",
      "   1.2622874   1.2384579]\n"
     ]
    }
   ],
   "source": [
    "# let's look at these concepts in action by writing a simple implementation of an mlp and its training loop\n",
    "from typing import Sequence, Union  # for Sequence: think list or tuple\n",
    "\n",
    "def simple_mlp(x:jax.Array, weights:Union[Sequence[jax.Array], jax.Array], biases:Union[Sequence[jax.Array], jax.Array]):\n",
    "    \"\"\"simple_mlp \n",
    "    Apply an MLP with weights and biases dicated by the weights and biases parameters, and ReLU activations, to the datapoint x.\n",
    "    The final layer of the MLP is a linear one.\n",
    "\n",
    "    :parameter x: jax.Array of shape (in_channels,)\n",
    "    :parameter weights: Either a jax.Array of shape (out_channels, in_channels), \n",
    "        or a Sequence of jax.Arrays, the first one of shape (hidden_channels_0, in_channels) \n",
    "        and the last one of shape (out_channels, hidden_channels_n)\n",
    "    :parameter biases: Either a jax.Array of shape (out_channels,),\n",
    "        or a Sequence of jax.Arrays, the first one of shape (hidden_channels_0,)\n",
    "        and the last one of shape (out_channels,)\n",
    "    :raises ValueError: in case the number of weights arrays isn't equal to the numer of biases arrays\n",
    "    :return: MLP applied to x\n",
    "    \"\"\"\n",
    "    # first do some case handling for the different input types (jax.Array vs Sequence[jax.Array])\n",
    "    if not isinstance(weights, Sequence):\n",
    "        weights = [weights]\n",
    "    if not isinstance(biases, Sequence):\n",
    "        biases = [biases]\n",
    "\n",
    "    # then check if we need to raise a ValueError\n",
    "    if len(weights) != len(biases):\n",
    "        raise ValueError(f\"weights and biases should have equal length. Got {len(weights)=} but {len(biases)=}.\")\n",
    "\n",
    "    # finally implement the logic of the MLP\n",
    "    # hint for the ReLU activations you can use jax.nn.relu\n",
    "    h = x \n",
    "    for w, b in zip(weights[:-1], biases[:-1]):\n",
    "        h = jax.nn.relu(w@h+b)\n",
    "    \n",
    "    w, b = weights[-1], biases[-1]\n",
    "    return w@h + b\n",
    "\n",
    "\n",
    "weights = (random.normal(next(key_gen), (16, 3)), random.normal(next(key_gen), (16, 16)), random.normal(next(key_gen), (1, 16)))  # NB this is really bad initialization\n",
    "biases = (random.normal(next(key_gen), (16,)), random.normal(next(key_gen), (16,)), random.normal(next(key_gen), (1,)))\n",
    "batch = random.normal(next(key_gen), (10, 3))\n",
    "\n",
    "# uncomment to test your code\n",
    "print(f\"{cju.debug_utils.summary(jax.vmap(simple_mlp, in_axes=(0, None, None))(batch, weights, biases))=}\")  # apply simple_mlp to a batch of inputs\n",
    "\n",
    "def calculate_loss(xs:jax.Array, ys:jax.Array, weights:tuple[jax.Array, ...], biases:tuple[jax.Array, ...]):\n",
    "    \"\"\"\n",
    "    apply simple_mlp on xs and compute the mean square error\n",
    "    :parameter xs: jax.Array with input data\n",
    "    :parameter ys: target values to be approximated by the mlp\n",
    "    :parameter weights: weigths of the MLP\n",
    "    :parameter biases: biases of the MLP\n",
    "    :return: scalar representing the mean squared error\n",
    "    \"\"\"\n",
    "    predictions = jax.vmap(simple_mlp, in_axes=(0, None, None))(xs, weights, biases)\n",
    "    return jnp.mean(jnp.square(ys-predictions))\n",
    "\n",
    "def simple_train_step(\n",
    "        weights_and_biases: tuple[Sequence[jax.Array], Sequence[jax.Array]], \n",
    "        data: tuple[jax.Array, jax.Array]\n",
    "        ) -> tuple[tuple[Sequence[jax.Array], Sequence[jax.Array]], jax.Array]:\n",
    "    \"\"\" \n",
    "    Perform a train step with a fixed learning rate of .01\n",
    "    The signature of this function (what kind of arguments it takes and what it returns) makes it so that it can be used with jax.lax.scan\n",
    "    :parameter weights_and_biases: 2-tuple with as its first element the weights of an MLP and as its second element the biases of an MLP (both compatible with)\n",
    "    :parameter data: 2-tuple with as its first element a jax.Array representing a batch of input data, and as its second element a jax.Array containing the corresponding labels\n",
    "    :return: 2-tuple, the first element of which contains a new value for weights_and_biases, and the second element of which contains the computed loss\n",
    "    \"\"\"\n",
    "    xs, ys = data\n",
    "    weights, biases = weights_and_biases\n",
    "    # hint: use jax.value_and_grad to get the loss and gradients, and use jax.tree.map for getting the updated weights and biases\n",
    "    loss, grads = jax.value_and_grad(\n",
    "        calculate_loss, \n",
    "        argnums=(2, 3)  # weights, biases\n",
    "    )(xs, ys, weights, biases)\n",
    "    new_weights, new_biases = jax.tree.map(\n",
    "        lambda w, g: w-.01*g,\n",
    "        (weights, biases),\n",
    "        grads\n",
    "    )\n",
    "    return (new_weights, new_biases), loss\n",
    "\n",
    "ys = random.normal(next(key_gen), (10, 1))\n",
    "\n",
    "training_data = random.normal(next(key_gen), shape=(20, 100, 3))  # 20 batches of size 100\n",
    "training_labels = jnp.sin(training_data)\n",
    "\n",
    "# uncomment the following to test your implementation\n",
    "print(f\"{cju.debug_utils.summary(simple_train_step((weights, biases), (batch, ys)))=}\")\n",
    "\n",
    "final_weights_and_biases, losses = jax.lax.scan(simple_train_step, init=(weights, biases), xs=(training_data, training_labels))\n",
    "print(cju.debug_utils.summary(final_weights_and_biases))\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, thinking of Pytrees just as compositions of Python containers and `jax.Array`s isn't quite right. The containers needn't be just standard Python containers, and the leafs of the tree needn't just be `jax.Array`s. Instead, what counts as a container for a pytree is any object whose type is registered in JAX's pytree container registry. And any object whose type is not in that registry is considered a leaf of a pytree. So, the definition given by the [JAX documentation](https://jax.readthedocs.io/en/latest/pytrees.html#what-is-a-pytree) is:\n",
    "1. any object whose type is *not* in the pytree container registry is considered a *leaf* pytree;\n",
    "1. any object whose type is in the pytree container registry, and which contains pytrees, is considered a pytree.\n",
    "\n",
    "[If you want, you can write and register your own pytree container types](https://jax.readthedocs.io/en/latest/pytrees.html#extending-pytrees). But likely you won't need to, as any subclass of `equinox.Module`, our next topic of interest, is automatically registered as a pytree container. Another source of pytrees that you will encounter is `optax` with its optimizer states and gradient updates.\n",
    "\n",
    "Utilities for working with pytrees can be found in [`jax.tree`](https://jax.readthedocs.io/en/latest/jax.tree.html), [`jax.tree_util`](https://jax.readthedocs.io/en/latest/jax.tree_util.html#module-jax.tree_util), [`common_jax_utils.tree_utils`](https://github.com/SimonKoop/common_jax_utils/blob/main/src/common_jax_utils/tree_utils.py), [`optax.tree_utils`](https://optax.readthedocs.io/en/latest/api/utilities.html#tree), and [`equinox`](https://docs.kidger.site/equinox/api/manipulation/).\n",
    "\n",
    "Equinox is particularly useful when some of the leafs in your pytree are not actually `jax.Array`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Equinox](https://docs.kidger.site/equinox/) tutorial\n",
    "\n",
    "Unlinke Pytorch and Tensorflow, JAX itself isn't a deeplearning library. There are various Neural Network libraries built ontop of JAX such as Flas, Haiku, and Equinox. In this course, we'll be using Equinox because it is conceptually simple (and frankly because I had a bunch of code for an old research project lying around that can make things significantly easier for you, and that code was written using Equinox). Importantly, Equinox makes it easy to do object oriented programming very similarly to Pytorch, but resulting in pytrees, so that the result is compatible with anything else in the JAX ecosystem.\n",
    "\n",
    "## 1. Equinox Modules\n",
    "Writing (neural network) models in Equinox is done by subclassing [`equinox.Module`](https://docs.kidger.site/equinox/api/module/module/). The resulting code looks quite similar to Pytorch code (where writing models happens through subclassing `torch.nn.Module`), but there are some key differences:\n",
    "* `equinox.Module`s are **static** (remember, to jit things, our functions need to be pure, i.e. without side-effects)\n",
    "* `equinox.Module`s are pytrees (they are registered as pytree containers automatically)\n",
    "* `equinox.Module`s are [*dataclasses*](https://docs.python.org/3/library/dataclasses.html) and all of their attributes need to be registered as *fields*. This also means they come with a default `__init__` method.\n",
    "* `equinox.Module`s will typically require a PRNG key for initializing weights and biases with random numbers\n",
    "* the forward pass of your model can be defined in its `__call__` method (or you can just use any other method you want).\n",
    "\n",
    "One particularly nice property of `equinox.Module`s, is that because they are pytrees, and because everything in JAX plays well with pytrees, you can have an `equinox.Module` that returns another `equinox.Module` in its forward pass, and everything will just work nicely the way you would want it to. We'll see this in the next practical session where we get more accustomed to the course code-base.\n",
    "\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  weights=[f32[16,3], f32[17,16], f32[16,17], f32[1,16]],\n",
      "  biases=[f32[16], f32[17], f32[16], f32[1]]\n",
      ")\n",
      "summary(jax.vmap(example_mlp)(x))='array(100, 1)'\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "from typing import Optional\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# let's first re-define simple_mlp from above as a eqx.Module\n",
    "# eqx.nn actually provides an MLP class, and moreover, it provides layers such as nn.Linear that you should ideally use for defining neural networks\n",
    "# but this does provide a good, very basic example for now\n",
    "class SimpleMLP(eqx.Module):\n",
    "    weights: Sequence[jax.Array]  # we need to register the attributes as fields\n",
    "    biases: Sequence[jax.Array]  # their values can only be set during initialization of the model\n",
    "\n",
    "    def __init__(self, in_size:int, out_size:int, hidden_size:Union[int, Sequence[int]], key:jax.Array, depth:Optional[int]=None):\n",
    "        \"\"\" \n",
    "        :parameter in_size: number of input features\n",
    "        :parameter out_size: number of output features\n",
    "        :parameter hidden_size: either an integer or a sequence of integers, the number of hidden units in each layer\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        :parameter depth: number of hidden layers (ignored if hidden_size is a sequence of integers)\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        if isinstance(hidden_size, int) and depth is not None:\n",
    "            hidden_size = depth*(hidden_size,)\n",
    "        elif isinstance(hidden_size, int):  # depth is None\n",
    "            raise ValueError(\"If hidden size is an integer, depth should be provided do determine the number of hidden layers\")\n",
    "        elif depth is not None: # hidden size is not an integer\n",
    "            warnings.warn(f\"The value provided for depth ({depth}) will be ignored because hidden_size is provided as a sequence ({hidden_size})\")\n",
    "\n",
    "        output_sizes = tuple(hidden_size) + (out_size,)\n",
    "        input_sizes = (in_size, ) + tuple(hidden_size)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for out_size, in_size in zip(output_sizes, input_sizes):\n",
    "            lim = 1/math.sqrt(in_size)\n",
    "            self.weights.append(random.uniform(\n",
    "                next(key_gen), \n",
    "                (out_size, in_size), \n",
    "                minval=-lim,\n",
    "                maxval=lim\n",
    "                ))\n",
    "            self.biases.append(random.uniform(\n",
    "                next(key_gen),\n",
    "                (out_size,),\n",
    "                minval=-lim,\n",
    "                maxval=lim\n",
    "            ))\n",
    "    \n",
    "    def __call__(self, x:jax.Array)->jax.Array:\n",
    "        \"\"\" \n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (in_size,)\n",
    "        :return: output of the MLP, jax.Array of shape (out_size,)\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            h = jax.nn.relu(w @ h + b)\n",
    "        w, b = self.weights[-1], self.biases[-1]\n",
    "        return w @ h + b\n",
    "    \n",
    "example_mlp = SimpleMLP(3, 1, (16, 17, 16), key=next(key_gen))\n",
    "print(example_mlp)\n",
    "\n",
    "x = random.normal(next(key_gen), (100, 3))\n",
    "print(f\"{summary(jax.vmap(example_mlp)(x))=}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For another example, let's now write a model that actually uses the compositional nature of `eqx.Module`s and uses some of the pre-defined layers from `eqx.nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCResnet(\n",
      "  input_layer=Linear(\n",
      "    weight=f32[32,3],\n",
      "    bias=f32[32],\n",
      "    in_features=3,\n",
      "    out_features=32,\n",
      "    use_bias=True\n",
      "  ),\n",
      "  res_blocks=[\n",
      "    FCResBlock(\n",
      "      norm_layer_0=LayerNorm(\n",
      "        shape=(32,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=True,\n",
      "        weight=f32[32],\n",
      "        bias=f32[32]\n",
      "      ),\n",
      "      norm_layer_1=LayerNorm(\n",
      "        shape=(32,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=True,\n",
      "        weight=f32[32],\n",
      "        bias=f32[32]\n",
      "      ),\n",
      "      linear_layer_0=Linear(\n",
      "        weight=f32[32,32],\n",
      "        bias=f32[32],\n",
      "        in_features=32,\n",
      "        out_features=32,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      linear_layer_1=Linear(\n",
      "        weight=f32[32,32],\n",
      "        bias=f32[32],\n",
      "        in_features=32,\n",
      "        out_features=32,\n",
      "        use_bias=True\n",
      "      )\n",
      "    ),\n",
      "    FCResBlock(\n",
      "      norm_layer_0=LayerNorm(\n",
      "        shape=(32,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=True,\n",
      "        weight=f32[32],\n",
      "        bias=f32[32]\n",
      "      ),\n",
      "      norm_layer_1=LayerNorm(\n",
      "        shape=(32,),\n",
      "        eps=1e-05,\n",
      "        use_weight=True,\n",
      "        use_bias=True,\n",
      "        weight=f32[32],\n",
      "        bias=f32[32]\n",
      "      ),\n",
      "      linear_layer_0=Linear(\n",
      "        weight=f32[32,32],\n",
      "        bias=f32[32],\n",
      "        in_features=32,\n",
      "        out_features=32,\n",
      "        use_bias=True\n",
      "      ),\n",
      "      linear_layer_1=Linear(\n",
      "        weight=f32[32,32],\n",
      "        bias=f32[32],\n",
      "        in_features=32,\n",
      "        out_features=32,\n",
      "        use_bias=True\n",
      "      )\n",
      "    )\n",
      "  ],\n",
      "  output_layer=Linear(\n",
      "    weight=f32[1,32],\n",
      "    bias=f32[1],\n",
      "    in_features=32,\n",
      "    out_features=1,\n",
      "    use_bias=True\n",
      "  )\n",
      ")\n",
      "summary(jax.vmap(my_resnet)(x))='array(100, 1)'\n"
     ]
    }
   ],
   "source": [
    "class FCResBlock(eqx.Module):\n",
    "    norm_layer_0: eqx.Module\n",
    "    norm_layer_1: eqx.Module\n",
    "    linear_layer_0: eqx.Module\n",
    "    linear_layer_1: eqx.Module\n",
    "\n",
    "    def __init__(self, feature_size, key):\n",
    "        \"\"\"\n",
    "        :parameter feature_size: number of features\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        self.norm_layer_0 = eqx.nn.LayerNorm(  # we are using LayerNorm out of convenience here\n",
    "            shape=(feature_size,)\n",
    "        )\n",
    "        self.norm_layer_1 = eqx.nn.LayerNorm(  # it's possible to use BatchNorm, but due to the stateful nature of BatchNorm, that would make this example slightly more involved\n",
    "            shape=(feature_size,)\n",
    "        )\n",
    "        self.linear_layer_0 = eqx.nn.Linear(\n",
    "            feature_size, feature_size,\n",
    "            key=next(key_gen)\n",
    "        )\n",
    "        self.linear_layer_1 = eqx.nn.Linear(\n",
    "            feature_size, feature_size,\n",
    "            key=next(key_gen)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x:jax.Array):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (feature_size,)\n",
    "        :return: output of the residual block, jax.Array of shape (feature_size,)\n",
    "        \"\"\"\n",
    "        h = self.norm_layer_0(x)\n",
    "        h = jax.nn.relu(h)\n",
    "        h = self.linear_layer_0(h)\n",
    "        h = self.norm_layer_1(h)\n",
    "        h = jax.nn.relu(h)\n",
    "        h = self.linear_layer_1(h)\n",
    "        return x + h\n",
    "    \n",
    "\n",
    "class FCResnet(eqx.Module):\n",
    "    input_layer: eqx.nn.Linear\n",
    "    res_blocks: list[FCResBlock]\n",
    "    output_layer: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, in_features:int, out_features:int, hidden_features:int, num_resblocks:int, key:jax.Array):\n",
    "        \"\"\"\n",
    "        :parameter in_features: number of input features\n",
    "        :parameter out_features: number of output features\n",
    "        :parameter hidden_features: number of features in the hidden layers\n",
    "        :parameter num_resblocks: number of residual blocks\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        self.input_layer = eqx.nn.Linear(in_features, hidden_features, key=next(key_gen))\n",
    "        self.res_blocks = [\n",
    "            FCResBlock(hidden_features, key=next(key_gen))\n",
    "            for _ in range(num_resblocks)\n",
    "        ]\n",
    "        self.output_layer = eqx.nn.Linear(hidden_features, out_features, key=next(key_gen))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (in_features,)\n",
    "        :return: output of the MLP, jax.Array of shape (out_features,)\n",
    "        \"\"\"\n",
    "        h = self.input_layer(x)\n",
    "        for block in self.res_blocks:\n",
    "            h = block(h)\n",
    "        return self.output_layer(h)\n",
    "    \n",
    "my_resnet = FCResnet(3, 1, 32, 2, key=next(key_gen))\n",
    "print(my_resnet)\n",
    "print(f\"{summary(jax.vmap(my_resnet)(x))=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "Write a ResNet class that uses [convolutional blocks](https://docs.kidger.site/equinox/api/nn/conv/#equinox.nn.Conv) and [batch normalization](https://docs.kidger.site/equinox/api/nn/normalisation/#equinox.nn.BatchNorm). Test it on some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "class ResBlock(eqx.Module):\n",
    "    norm_layer_0: eqx.nn.BatchNorm\n",
    "    norm_layer_1: eqx.nn.BatchNorm\n",
    "    conv_layer_0: eqx.Module\n",
    "    conv_layer_1: eqx.Module\n",
    "\n",
    "    def __init__(self, feature_size:int, kernel_size:int, num_spatial_dims:int, key:jax.Array):\n",
    "        \"\"\"\n",
    "        :parameter feature_size: number of features\n",
    "        :parameter kernal_size: size of the convolutional kernel\n",
    "        :num_spatial_dims: how many dimensions to convolve over. E.g. 1 leads to Conv1d, 2 to Conv2d etc.\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        self.norm_layer_0 = eqx.nn.BatchNorm(\n",
    "            input_size=feature_size,\n",
    "            axis_name='batch'\n",
    "        )\n",
    "        self.norm_layer_1 = eqx.nn.BatchNorm(\n",
    "            input_size=feature_size,\n",
    "            axis_name='batch'\n",
    "        )\n",
    "        self.conv_layer_0 = eqx.nn.Conv(\n",
    "            num_spatial_dims,\n",
    "            feature_size, feature_size,\n",
    "            kernel_size,\n",
    "            padding='same',\n",
    "            key=next(key_gen)\n",
    "        )\n",
    "        self.conv_layer_1 = eqx.nn.Conv(\n",
    "            num_spatial_dims,\n",
    "            feature_size, feature_size,\n",
    "            kernel_size,\n",
    "            padding='same',\n",
    "            key=next(key_gen)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x:jax.Array, state:eqx.nn.State)->tuple[jax.Array, eqx.nn.State]:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (feature_size,)\n",
    "        :parameter state: eqx.nn.State containing the statistics for batch normalization\n",
    "        :return: a 2-tuple, the first element of which is the output of the residual block, \n",
    "            and the second element is the updated state \n",
    "        \"\"\"\n",
    "        h, state = self.norm_layer_0(x, state)\n",
    "        h = jax.nn.relu(h)\n",
    "        h = self.conv_layer_0(h)\n",
    "        h, state = self.norm_layer_1(h, state)\n",
    "        h = jax.nn.relu(h)\n",
    "        h = self.conv_layer_1(h)\n",
    "        return x + h, state\n",
    "    \n",
    "\n",
    "class ResNet(eqx.Module):\n",
    "    input_layer: eqx.nn.Linear\n",
    "    res_blocks: list[FCResBlock]\n",
    "    output_layer: eqx.nn.Linear\n",
    "    pooling_dims: tuple[int]\n",
    "\n",
    "    def __init__(self, in_features:int, out_features:int, hidden_features:int, num_resblocks:int, kernel_size:int, num_spatial_dims:int, key:jax.Array):\n",
    "        \"\"\"\n",
    "        :parameter in_features: number of input features\n",
    "        :parameter out_features: number of output features\n",
    "        :parameter hidden_features: number of features in the hidden layers\n",
    "        :parameter num_resblocks: number of residual blocks\n",
    "        :parameter kernel_size: size of the convolutional kernels\n",
    "        :parameter num_spatial_dims: number of spatial dimensions (e.g 1 for using Conv1d, 2 for Conv2d, etc.)\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        self.input_layer = eqx.nn.Conv(num_spatial_dims, in_features, hidden_features, kernel_size, padding='same', key=next(key_gen))\n",
    "        self.res_blocks = [\n",
    "            ResBlock(hidden_features, kernel_size, num_spatial_dims, key=next(key_gen))\n",
    "            for _ in range(num_resblocks)\n",
    "        ]\n",
    "        self.output_layer = eqx.nn.Linear(hidden_features, out_features, key=next(key_gen))\n",
    "\n",
    "        # channels are on axis 0, the rest are spatial/temporal channels over which to pool before using the linear layer\n",
    "        self.pooling_dims = tuple(range(1, num_spatial_dims+1))  \n",
    "\n",
    "        # NB channels are on axis 0 because we vmap over the batch dimension, so we write our code for a single data point instead of for a batch\n",
    "        \n",
    "\n",
    "    def __call__(self, x:jax.Array, state:eqx.nn.State)->tuple[jax.Array, eqx.nn.State]:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (in_features,)\n",
    "        :parameter state: eqx.nn.State containing the statistics for batch normalization\n",
    "        :return: 2-tuple, the first element of which is the output of the residual network, i.e. an (out_features,) shaped jax.Array, \n",
    "            and the second element is the updated state.\n",
    "        \"\"\"\n",
    "        h = self.input_layer(x)\n",
    "        for block in self.res_blocks:\n",
    "            h, state = block(h, state)\n",
    "        h_averaged = h.mean(axis=self.pooling_dims)\n",
    "        return self.output_layer(h_averaged), state\n",
    "    \n",
    "\n",
    "example_model, state = eqx.nn.make_with_state(ResNet)(\n",
    "    in_features=3,\n",
    "    out_features=9,\n",
    "    hidden_features=128,\n",
    "    num_resblocks=3,\n",
    "    kernel_size=3,\n",
    "    num_spatial_dims=1,\n",
    "    key=next(key_gen)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('array(20, 9)', State(\n",
      "  0x7b04d61000d0='array()',\n",
      "  0x7b04d61000f0=('array(128,)', 'array(128,)'),\n",
      "  0x7b04d6100110='array()',\n",
      "  0x7b04d6100130=('array(128,)', 'array(128,)'),\n",
      "  0x7b04d6100150='array()',\n",
      "  0x7b04d6100170=('array(128,)', 'array(128,)'),\n",
      "  0x7b04d6100190='array()',\n",
      "  0x7b04d61001b0=('array(128,)', 'array(128,)'),\n",
      "  0x7b04d61001d0='array()',\n",
      "  0x7b04d61001f0=('array(128,)', 'array(128,)'),\n",
      "  0x7b04d6100210='array()',\n",
      "  0x7b04d6100230=('array(128,)', 'array(128,)')\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "example_data = random.normal(next(key_gen), (20, 3, 32))\n",
    "\n",
    "print(summary(jax.vmap(example_model, in_axes=(0, None), out_axes=(0, None), axis_name='batch')(example_data, state)))  # Don't forget the `out_axes=(0, None)`!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtered transformations\n",
    "When the only leafs of our `eqx.Module`s are `jax.Array`s, we can use JAX's built in function transformations like `vmap`, `grad`, and `jit` without any problems, because, as we saw before, those play nice with pytrees. You can see this in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.93261236, dtype=float32),\n",
       " SimpleMLP(\n",
       "   weights=[f32[16,3], f32[17,16], f32[16,17], f32[1,16]],\n",
       "   biases=[f32[16], f32[17], f32[16], f32[1]]\n",
       " ))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(network, x, y):\n",
    "    return jnp.mean(jnp.square(jax.vmap(network)(x)-y))\n",
    "\n",
    "jitted_value_and_grad_loss = jax.jit(jax.value_and_grad(loss))\n",
    "jitted_value_and_grad_loss(example_mlp, x, random.normal(next(key_gen), (100, 1)))  # note that the grad has the same structure as the network, so it's a SimpleMLP itself!\n",
    "# (albeit not one that we would want to apply to data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for many models, we might want to store more information in the model than just arrays containing weights and biases. E.G. maybe we want to have some freedom in which activation function is used, and we want to store it as an attribute. This can cause some problems if we just use `jax.grad` or `jax.jit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class NNLayer(eqx.Module):\n",
    "    weights: jax.Array\n",
    "    biases: jax.Array\n",
    "    activation_function: Callable\n",
    "\n",
    "    def __init__(self, in_features:int, out_features:int, key:jax.Array, activation_function:Callable=jax.nn.relu):\n",
    "        \"\"\"\n",
    "        :parameter in_features: number of input features\n",
    "        :parameter out_features: number of output features\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        :parameter activation_function: activation function to be used\n",
    "        \"\"\"\n",
    "        lim = 1/math.sqrt(in_features)\n",
    "        self.weights = random.uniform(\n",
    "            key, \n",
    "            (out_features, in_features), \n",
    "            minval=-lim,\n",
    "            maxval=lim\n",
    "            )\n",
    "        self.biases = random.uniform(\n",
    "            key,\n",
    "            (out_features,),\n",
    "            minval=-lim,\n",
    "            maxval=lim\n",
    "        )\n",
    "        self.activation_function = activation_function\n",
    "    \n",
    "    def __call__(self, x:jax.Array, *, key:Optional[jax.Array]=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (in_features,)\n",
    "        :parameter key: ignored, provided for compatibility with the Equinox API (so we can use eqx.nn.Sequential later on)\n",
    "        :return: output of the layer, jax.Array of shape (out_features,)\n",
    "        \"\"\"\n",
    "        return self.activation_function(self.weights@x+self.biases)\n",
    "\n",
    "my_layer = NNLayer(3, 1, key=next(key_gen))\n",
    "# uncomment the following line to try it out and get the TypeError:\n",
    "# jitted_value_and_grad_loss(my_layer, x, random.normal(next(key_gen), (100, 1)))  # TypeError: Cannot interpret value of type <class 'jax._src.custom_derivatives.custom_jvp'> as an abstract array; it does not have a dtype attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to deal with this, would be to manually indicate whicharguments should be treated statically. Doing so would however be rather inconvenient. Fortunately, equinox provides tools to deal with this instead: [filtered versions of many jax transformations](https://docs.kidger.site/equinox/api/transformations/). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.9786879, dtype=float32),\n",
       " NNLayer(weights=f32[1,3], biases=f32[1], activation_function=None))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_jvag_loss = eqx.filter_jit(eqx.filter_value_and_grad(loss))\n",
    "filtered_jvag_loss(my_layer, x, random.normal(next(key_gen), (100, 1)))  \n",
    "# question: my_layer works on 3-vectors, but x has shape (100, 3), why shouldn't we put jax.vmap(my_layer) instead of my_layer in filtered_jvag_loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optax](https://optax.readthedocs.io/en/latest/index.html) tutorial\n",
    "Earlier in this notebook, we trained a neural network (our `simple_mlp`) by applyin a gradient update sort of manually using `jax.tree.map`:\n",
    "```python\n",
    "new_weights, new_biases = jax.tree.map(\n",
    "    lambda w, g: w-.01*g,  # gradient descent with learning rate 0.01\n",
    "    (weights, biases),\n",
    "    grads\n",
    ")\n",
    "```\n",
    "In general however, we might want to modify the gradients slightly, e.g. by clipping them, or we might want to use optimization techniques such as momentum. \n",
    "\n",
    "In Pytorch, we have `Optimizer` objects that track state like momentum, and apply the correct updates to the model. In JAX however, we want operations to have no side-effects so that we can JIT them. That means we'll have to keep track of any state (such as momentum) explicitly, and we will have to combine our model with any updates into a new model since the model can't be changed (as that would be a side-effect).\n",
    "\n",
    "Nonetheless, we don't have to do all of the work of transforming the gradients in combination with a state into updates to our network and a new state manually every time. Optax is an optimization library for JAX that implements many commonly used optimizers, such as the Adam optimizer.\n",
    "\n",
    "The first core concept of Optax is that of the `GradientTransformation`. A `GradientTransformation` object contains an `update` method that takes gradients, a state, and (optionally) the current parameters that are to be updated, and transforms them into updates for the model and a new state. One example of such a transformation is gradient clipping ([`optax.clip`](https://optax.readthedocs.io/en/latest/api/transformations.html#optax.clip)): it takes a pytree of gradients and a state (and optionally the parameters), and returns a clipped version of the pytree of gradients and the same state.\n",
    "\n",
    "Another example is ['optax.adam'](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.adam): the resulting `GradientTransformation` takes a pytree of gradients, an *appropriate* state (a.o. containing a $1^{\\text{st}}$ moment pytree and a $2^{\\text{nd}}$ moment pytree), and optionally a pytree of parameters, and computes new moments for the state and returns updates based on those new moments and the new state. \n",
    "\n",
    "These gradient transformations are composable, so that we can combine the transformation from `optax.clip` with that from `optax.adam` into a `GradientTransformation` that first clips the gradients and then applies the updaterules from Adam.\n",
    "\n",
    "Moreover, they don't only implement their update rule (in their `update` method), but they also implement an `init` method that takes a pytree of model parameters (e.g. an `eqx.Module`) and returns an appropriate optimizer state (an instance of `OptState`, which makes it a pytree as well).\n",
    "\n",
    "**In summary**, to use optimizers from Optax, what we need to do is\n",
    "1. define a model, say `model` (e.g. an `eqx.Module`)\n",
    "1. create a `GradientTransform`, say `optimizer` (e.g. by calling `optax.adam(learning_rate=.01)`)\n",
    "1. create an appropriate `OptState` by setting `opt_state = optimizer.init(model)`\n",
    "\n",
    "and then during our training loop, at each step we\n",
    "1. compute the gradients of the loss (and the loss itself) using `jax.value_and_grad` or `eqx.filter_value_and_grad`\n",
    "1. we combine the gradients and the existing optimizer state into updates and a new optimizer state (think `updates, opt_state = optimizer.update(grads, opt_state, model)`)\n",
    "1. we combine our existing model and the updates into a new model\n",
    "\n",
    "This last step can be done either by using `optax.apply_updates` or [`eqx.apply_updates`](https://docs.kidger.site/equinox/api/manipulation/#equinox.apply_updates) (or by using `jax.tree.map` to apply the updates manually). When working with `eqx.Module`s, it's typically best to use `eqx.apply_updates` for basically the same reason as why we want to use `eqx.filter_grad` and `eqx.filter_jit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43557435 0.4525172  0.40632796 0.43212205 0.443473   0.3904306\n",
      " 0.37801644 0.37055156 0.34726283 0.3081857  0.35607505 0.31400898\n",
      " 0.33122024 0.2920717  0.29022804 0.32155767 0.3180481  0.29130617\n",
      " 0.2987887  0.28051388]\n"
     ]
    }
   ],
   "source": [
    "# let's train example_mlp from earlier \n",
    "import optax \n",
    "\n",
    "\n",
    "optimizer = optax.adam(.01)\n",
    "initial_state = optimizer.init(eqx.filter(example_mlp, eqx.is_array))\n",
    "# the filter isn't really necessary for example_mlp, but it is for any model that contains\n",
    "# non-array attributes, such as the NNLayer class above\n",
    "\n",
    "def loss_func(model, xs, ys):\n",
    "    predictions = jax.vmap(model)(xs)\n",
    "    return jnp.mean(jnp.square(predictions-ys))\n",
    "\n",
    "#@eqx.filter_jit  # see text after this cell\n",
    "def train_step(model_and_state, data):\n",
    "    model, opt_state = model_and_state\n",
    "    xs, ys = data\n",
    "    loss, grads = eqx.filter_value_and_grad(loss_func)(model, xs, ys)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state, model)\n",
    "    new_model = eqx.apply_updates(model, updates)\n",
    "    return (new_model, new_opt_state), loss\n",
    "\n",
    "\n",
    "# now we train the model\n",
    "example_mlp_trained, losses = jax.lax.scan(train_step, init=(example_mlp, initial_state), xs=(training_data, training_labels))\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 100, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note about the commented-out `@eqx.filter_jit` above the definition of `train_step` in the above example: in this example we use `jax.lax.scan` for the training loop. As mentioned in the \"Note\" in the [documentation of `jax.lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan), `scan()` compiles the function that is scanned, so it is unnecessary to manually jit `train_step` in this case.\n",
    "\n",
    "However, in practice, we often want to do more than just train a model during our training loop: we might want to give live updates about the loss and about various metrics to [\"weights and biases\" (wandb)](https://wandb.ai), and we might want to run a validation loop after every `N` steps, and maybe store some intermediate results, etc. This is much easier when we just run a Python `for` loop, calling the `train_step` function at every iteration. \n",
    "\n",
    "You may wonder: do we lose anything by running a Python `for` loop instead of using `jax.lax.scan`? Afterall, Python loops are slow, right? In all honesty, I don't know for sure. You may want to experiment with this. If I had to give an educated guess however, I would say: as long as your model is large enough, you shouldn't miss anything. Or more precisely: as long as a single train step takes longer for the GPU to compute than that the python interpreter takes to interpret the (hopefully hand full of) lines in the training loop, [asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#async-dispatch) should mean that there is no real overhead of the python interpreter.\n",
    "\n",
    "So then in the usual case, where your train loop is just a Python `for` loop, **what functions should you jit?** Basically, only the `train_step` (and maybe e.g. a `validation_step` if you have implemented it). *As a rule of thumb, you want to jit the largest possible function for the best result.* So **don't** jit your model and your loss function etc. separately, passing a jitted model to the jitted `value_and_grad` of a jitted loss function. But instead **only jit the function that combines all of them** (i.e. the `train_step`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "Define a model using `equinox.nn.Sequential` and using the either `NNLayer`s defined earlier in this notebook or your own adaptation thereof. Define a `train_step` function, and train your model using [`optax.amsgrad`](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.amsgrad) for 20 000 steps on the data provided in the next cell, and print the loss every 400 steps. After every 2 000 steps, compute the loss of your model on the validation data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: Average loss over past 400 steps is 1.308192, most recent loss was 1.143559\n",
      "Step 800: Average loss over past 400 steps is 1.288161, most recent loss was 1.004046\n",
      "Step 1200: Average loss over past 400 steps is 1.277691, most recent loss was 1.197715\n",
      "Step 1600: Average loss over past 400 steps is 1.270541, most recent loss was 1.186588\n",
      "Step 2000: Average loss over past 400 steps is 1.264957, most recent loss was 1.117419\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.276634\n",
      "\n",
      "Step 2400: Average loss over past 400 steps is 1.260207, most recent loss was 0.988892\n",
      "Step 2800: Average loss over past 400 steps is 1.255895, most recent loss was 1.213949\n",
      "Step 3200: Average loss over past 400 steps is 1.251733, most recent loss was 1.019403\n",
      "Step 3600: Average loss over past 400 steps is 1.247568, most recent loss was 0.966704\n",
      "Step 4000: Average loss over past 400 steps is 1.243258, most recent loss was 1.208685\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.256798\n",
      "\n",
      "Step 4400: Average loss over past 400 steps is 1.238754, most recent loss was 1.357954\n",
      "Step 4800: Average loss over past 400 steps is 1.234066, most recent loss was 1.475914\n",
      "Step 5200: Average loss over past 400 steps is 1.229282, most recent loss was 1.165369\n",
      "Step 5600: Average loss over past 400 steps is 1.224508, most recent loss was 1.109937\n",
      "Step 6000: Average loss over past 400 steps is 1.219937, most recent loss was 0.970838\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.236145\n",
      "\n",
      "Step 6400: Average loss over past 400 steps is 1.215690, most recent loss was 1.089620\n",
      "Step 6800: Average loss over past 400 steps is 1.211886, most recent loss was 0.963160\n",
      "Step 7200: Average loss over past 400 steps is 1.208509, most recent loss was 1.518245\n",
      "Step 7600: Average loss over past 400 steps is 1.205532, most recent loss was 1.403950\n",
      "Step 8000: Average loss over past 400 steps is 1.202890, most recent loss was 1.314329\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.221101\n",
      "\n",
      "Step 8400: Average loss over past 400 steps is 1.200560, most recent loss was 1.360113\n",
      "Step 8800: Average loss over past 400 steps is 1.198498, most recent loss was 0.839422\n",
      "Step 9200: Average loss over past 400 steps is 1.196656, most recent loss was 1.024777\n",
      "Step 9600: Average loss over past 400 steps is 1.195006, most recent loss was 1.284970\n",
      "Step 10000: Average loss over past 400 steps is 1.193537, most recent loss was 1.584916\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.212631\n",
      "\n",
      "Step 10400: Average loss over past 400 steps is 1.192210, most recent loss was 1.009513\n",
      "Step 10800: Average loss over past 400 steps is 1.191046, most recent loss was 1.307416\n",
      "Step 11200: Average loss over past 400 steps is 1.189968, most recent loss was 1.387255\n",
      "Step 11600: Average loss over past 400 steps is 1.188988, most recent loss was 1.041941\n",
      "Step 12000: Average loss over past 400 steps is 1.188099, most recent loss was 1.321977\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.207496\n",
      "\n",
      "Step 12400: Average loss over past 400 steps is 1.187269, most recent loss was 1.473500\n",
      "Step 12800: Average loss over past 400 steps is 1.186501, most recent loss was 1.140327\n",
      "Step 13200: Average loss over past 400 steps is 1.185787, most recent loss was 1.283753\n",
      "Step 13600: Average loss over past 400 steps is 1.185133, most recent loss was 1.222238\n",
      "Step 14000: Average loss over past 400 steps is 1.184512, most recent loss was 1.111185\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.204080\n",
      "\n",
      "Step 14400: Average loss over past 400 steps is 1.183943, most recent loss was 1.234973\n",
      "Step 14800: Average loss over past 400 steps is 1.183411, most recent loss was 1.119403\n",
      "Step 15200: Average loss over past 400 steps is 1.182927, most recent loss was 1.216464\n",
      "Step 15600: Average loss over past 400 steps is 1.182471, most recent loss was 1.273849\n",
      "Step 16000: Average loss over past 400 steps is 1.182037, most recent loss was 1.408268\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.201807\n",
      "\n",
      "Step 16400: Average loss over past 400 steps is 1.181642, most recent loss was 1.253214\n",
      "Step 16800: Average loss over past 400 steps is 1.181271, most recent loss was 1.520153\n",
      "Step 17200: Average loss over past 400 steps is 1.180924, most recent loss was 1.073908\n",
      "Step 17600: Average loss over past 400 steps is 1.180596, most recent loss was 1.131625\n",
      "Step 18000: Average loss over past 400 steps is 1.180287, most recent loss was 1.221079\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.200136\n",
      "\n",
      "Step 18400: Average loss over past 400 steps is 1.180011, most recent loss was 1.090996\n",
      "Step 18800: Average loss over past 400 steps is 1.179746, most recent loss was 1.026219\n",
      "Step 19200: Average loss over past 400 steps is 1.179498, most recent loss was 1.006925\n",
      "Step 19600: Average loss over past 400 steps is 1.179260, most recent loss was 1.076446\n",
      "Step 20000: Average loss over past 400 steps is 1.179035, most recent loss was 1.156059\n",
      "\n",
      "Starting validation loop\n",
      "Validation loss: 1.198859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = random.normal(next(key_gen), shape=(40000, 3))\n",
    "training_labels = jnp.sin(training_data[:, 0]) - jnp.cos(training_data[:, 1]) + jnp.sin(training_data[:, 2] - 0.1*training_data[:, 0] + 0.1*training_data[:, 1])\n",
    "training_labels = training_labels[:, None]\n",
    "\n",
    "validation_data = random.normal(next(key_gen), shape=(2000, 3))\n",
    "validation_labels = jnp.sin(validation_data[:, 0]) - jnp.cos(validation_data[:, 1]) + jnp.sin(validation_data[:, 2] - 0.1*validation_data[:, 0] + 0.1*validation_data[:, 1])\n",
    "validation_labels = validation_labels[:, None]\n",
    "\n",
    "# your code goes here\n",
    "\n",
    "# first we define our model, optimizer, and initial state\n",
    "my_model = eqx.nn.Sequential([\n",
    "    NNLayer(3, 256, next(key_gen), jax.nn.celu),\n",
    "    NNLayer(256, 256, next(key_gen), jax.nn.celu),\n",
    "    NNLayer(256, 256, next(key_gen), jax.nn.celu),\n",
    "    NNLayer(256, 1, next(key_gen))\n",
    "])\n",
    "\n",
    "optimizer = optax.amsgrad(2e-6)\n",
    "\n",
    "optimizer_state = optimizer.init(eqx.filter(my_model, eqx.is_array))\n",
    "\n",
    "# then we define our train step and validation step\n",
    "@eqx.filter_jit\n",
    "def train_step(model, xs, ys, opt_state):\n",
    "    loss, grads = eqx.filter_value_and_grad(loss_func)(model, xs, ys)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state, model)\n",
    "    new_model = eqx.apply_updates(model, updates)\n",
    "    return new_model, new_opt_state, loss\n",
    "\n",
    "@eqx.filter_jit\n",
    "def validation_step(model, xs, ys):\n",
    "    loss = loss_func(model, xs, ys)\n",
    "    return loss\n",
    "\n",
    "# we define an iterator that just keeps iterating over batches of training data (shuffeling them each time we've had the whole data set)\n",
    "def my_iterator(data:jax.Array, labels:jax.Array, batch_size:int, key:jax.Array):\n",
    "    \"\"\" \n",
    "    :parameter data: data to iterate over\n",
    "    :parameter labels: corresponding labels\n",
    "    :parameter batch_size: size of the batches that are to be created\n",
    "    :parameter key: jax prng key\n",
    "\n",
    "    :yields: tuples of two `jax.Array`s, the first of which is a batch of data, and the second of which is the corresponding batch of labels\n",
    "    \"\"\"\n",
    "    key_gen = cju.key_generator(key)\n",
    "    num_batches = data.shape[0]//batch_size\n",
    "    num_data_points_per_epoch = num_batches*batch_size\n",
    "\n",
    "    while True:\n",
    "        index_permutation = random.permutation(next(key_gen), data.shape[0])  # we to re-shuffle the data every time we loop over it\n",
    "        data_batches = data[index_permutation[:num_data_points_per_epoch]].reshape((num_batches, batch_size, )+data.shape[1:])\n",
    "        label_batches = labels[index_permutation[:num_data_points_per_epoch]].reshape((num_batches, batch_size, )+labels.shape[1:])\n",
    "        yield from zip(data_batches, label_batches)\n",
    "\n",
    "# define the batch size and number of training steps\n",
    "batch_size = 100\n",
    "num_steps = 20_000\n",
    "num_val_batches = validation_data.shape[0]//batch_size\n",
    "\n",
    "# and now for the training loop\n",
    "train_losses = []\n",
    "for step_index, (train_batch, train_labels) in zip(\n",
    "    range(1, num_steps+1), # start counting at 1\n",
    "    my_iterator(training_data, training_labels, batch_size, next(key_gen))  # it's okay that this is infinite, because the range isn't\n",
    "    ):\n",
    "    my_model, optimizer_state, loss = train_step(my_model, train_batch, train_labels, optimizer_state)\n",
    "    train_losses.append(loss)\n",
    "\n",
    "    if step_index % 400 == 0:  # here it helps that we started counting at 1\n",
    "        print(f\"Step {step_index}: Average loss over past 400 steps is {np.mean(train_losses):.6f}, most recent loss was {loss:.6f}\")\n",
    "        train_losses = []\n",
    "    \n",
    "    if step_index % 2_000 == 0:\n",
    "        print(\"\\nStarting validation loop\")\n",
    "        validation_losses = []\n",
    "        for index in range(num_val_batches):\n",
    "            val_batch = validation_data[index*batch_size : (index+1)*batch_size]\n",
    "            val_labels = validation_labels[index*batch_size : (index+1)*batch_size]\n",
    "            loss = validation_step(my_model, val_batch, val_labels)\n",
    "            validation_losses.append(loss)\n",
    "        print(f\"Validation loss: {np.mean(validation_losses):.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of additional resources\n",
    "Here is a (far from comprehensive) list of additional material to look at if you get stuck with anything JAX related:\n",
    "* https://docs.kidger.site/equinox/faq/  answers to questions you'll likely at some point have (e.g. \"How to mark arrays as non-trainable?\")\n",
    "* https://docs.kidger.site/equinox/api/serialisation/  for storing your Equinox models\n",
    "* https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html for more automatic differentiation than just `jax.grad`\n",
    "* https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html for information on problems you're likely to run into\n",
    "* https://jax.readthedocs.io/en/latest/faq.html for more answers to questions you might have at some point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
