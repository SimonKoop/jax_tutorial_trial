{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/SimonKoop/jax_tutorial_trial/blob/main/practical_session_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# JAX Tutorial 1\n",
    "## What's JAX?\n",
    "* Accellerated Array computations (like Numpy but on GPU)\n",
    "* JIT (just-in-time) compiled to XLA (Accellerated Linear Algebra)\n",
    "* Autograd and other transformations\n",
    "\n",
    "\n",
    "jax.numpy: stand-in replacement for numpy\n",
    "\n",
    "NB some caveats apply! More on that later, but also, see [this page](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html) for a more comprehensive guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should install all requirements if you're running this in colab\n",
    "#%pip install git+https://github.com/SimonKoop/common_jax_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "def sigmoid(x:jax.Array)->jax.Array:  # many commonly used activation functions can be found in jax.nn (e.g. jax.nn.sigmoid)\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "some_vector = jnp.array([-1., 0., 1., 2.])\n",
    "print(sigmoid(some_vector))\n",
    "\n",
    "# we can JIT compile the functions\n",
    "sigmoid_jitted = jax.jit(sigmoid)\n",
    "print(sigmoid_jitted(some_vector))  # first time calling this will take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sigmoid(some_vector).block_until_ready()  # if you want to know why we use block_until_ready(), read https://jax.readthedocs.io/en/latest/async_dispatch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sigmoid_jitted(some_vector).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Use the tools from jax.numpy to compute the following:\n",
    "$$\n",
    "\\mathrm{sigmoid}\\left(\\left(\\begin{matrix}1. & 0.5 & 0.25\\\\ 0. & 2. & 0.25 \\\\ 0. & 0. & 1.\\end{matrix}\\right)\\left(\\begin{matrix}1. \\\\-1. \\\\ .3\\end{matrix}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First quirk: pure functions\n",
    "In order to make a.o. JIT compilation and automatic differentiation easier, the creators of JAX opted for a more functional style of programming. For many of the useful transforms of JAX to work, you need to use **pure functions** i.e. functions without side-effects.\n",
    "\n",
    "Because of this, **jax arrays are immutable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax arrays vs numpy arrays example\n",
    "import numpy as np\n",
    "\n",
    "some_numpy_array = np.zeros((3, 3))\n",
    "some_numpy_array[1] = np.linspace(0., 1., 3)\n",
    "print(f\"{some_numpy_array=}\")\n",
    "\n",
    "some_jax_array = jnp.zeros((3, 3))\n",
    "# some_jax_array[1] = jnp.linspace(0., 1., 3)  # this will give an error, uncomment if you want to try\n",
    "# print(f\"{some_jax_array=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to do this instead\n",
    "some_new_jax_array = some_jax_array.at[1].set(jnp.linspace(0., 1., 3))\n",
    "print(f\"{some_jax_array=}\")  # also, note that the default dtype is jnp.float32. \n",
    "print(f\"{some_new_jax_array=}\")  # To be able to use float64, you'd have to enable it at startup, but maybe just don't unless you really need to.\n",
    "\n",
    "# Note that the original array remains unmodified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, if we want to use function transformations like `jax.jit` or `jax.grad` (more on this later), we will have to use pure functions. To gain some understanding of what this means, let's look at something that is *not* a pure function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following works absolutely fine untill we try to JIT things (or use autograd, or any of the other useful function transformations)\n",
    "class SelfChangingLayer:\n",
    "\n",
    "    def __init__(self, initial_weights:jax.Array):\n",
    "        \"\"\" \n",
    "        Initialize the SelfChangingLayer\n",
    "        :parameter initial_weights: weights matrix, is.e. jax.Array with shape (N, N)\n",
    "        \"\"\"\n",
    "        self.weights = initial_weights\n",
    "\n",
    "    def __call__(self, x:jax.Array):\n",
    "        \"\"\" \n",
    "        forward pass\n",
    "        :parameter x: vector i.e. jax.Array with shape (N,)\n",
    "        \"\"\"\n",
    "        output = self.weights@x\n",
    "        # and now for the side-effect:\n",
    "        self.weights = .9*self.weights + .1*(output[:, None]@output[None, :])  # output[:, None] has shape (N, 1) and output[None, :] has shape (1, N)\n",
    "        return output\n",
    "    \n",
    "my_layer = SelfChangingLayer(jnp.eye(3))\n",
    "some_x1 = jnp.array([1., 0., 0.])\n",
    "some_x2 = jnp.array([2., -1., 2.])\n",
    "\n",
    "print(f\"{my_layer(some_x1)=}\")\n",
    "print(f\"{my_layer(some_x2)=}\")\n",
    "print(f\"{my_layer(some_x1)=}\")\n",
    "\n",
    "# As you can see, my_layer has side-effects. Due to this, the same input results in two different outputs.\n",
    "# That means we can't jit compile it, or use automatic differentiation with it, without getting wrong answers\n",
    "\n",
    "my_layer_jitted = jax.jit(SelfChangingLayer(jnp.eye(3)))\n",
    "print(f\"{my_layer_jitted(some_x1)=}\")\n",
    "print(f\"{my_layer_jitted(some_x2)=}\")\n",
    "print(f\"{my_layer_jitted(some_x1)=}\")  # as you can see, these outcomes are not correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The way we typically deal with this, is by making the state an argument to the function and returning an updated state as an output of the function. Write a pure function analogue of the `SelfChangingLayer` above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_changing_layer(x:jax.Array, weights:jax.Array) -> tuple[jax.Array, jax.Array]:\n",
    "    # your code goes here\n",
    "    pass\n",
    "\n",
    "# uncomment the following to test your code\n",
    "\n",
    "# weights = jnp.eye(3)\n",
    "# output_1, weights = self_changing_layer(some_x1, weights)\n",
    "# output_2, weights = self_changing_layer(some_x2, weights)\n",
    "# output_3, weights = self_changing_layer(some_x1, weights)\n",
    "# print(f\"{output_1=}\\n{output_2=}\\n{output_3=}\")\n",
    "\n",
    "# self_changing_layer_jitted = jax.jit(self_changing_layer)\n",
    "# weights = jnp.eye(3)\n",
    "# output_1, weights = self_changing_layer_jitted(some_x1, weights)\n",
    "# output_2, weights = self_changing_layer_jitted(some_x2, weights)\n",
    "# output_3, weights = self_changing_layer_jitted(some_x1, weights)\n",
    "# print(f\"{output_1=}\\n{output_2=}\\n{output_3=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this mean we can't use object-oriented programming with JAX? No, later in this tutorial we'll look at a package called Equinox that will help us with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher order functions\n",
    "Another aspect of functional programming that JAX makes heavy use of, is higher order functions. That means: functions that either take another function as an argument, or return another function as their output (or both). One example we have already seen of a higher order function is `jax.jit`: it takes a (pure) function as its input, and returns a jit-compiled function as its output. We also alluded to the the existence of `jax.grad`, let's now look at what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_grad = jax.grad(sigmoid)  # this gives the gradient of sigmoid with respect to its first (and only) argument. \n",
    "def sigmoid_grad_manual(x):\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x*(1-sigmoid_x)\n",
    "\n",
    "print(f\"0.: {sigmoid_grad(0.)}, 1.:{sigmoid_grad(1.)}, -1.:{sigmoid_grad(-1.)}\")\n",
    "print(f\"0.: {sigmoid_grad_manual(0.)}, 1.:{sigmoid_grad_manual(1.)}, -1:{sigmoid_grad_manual(-1.)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One salient detail of `jax.grad` is that the function of which we compute the gradient must have scalar output. \n",
    "When we feed `sigmoid` a vector, it returns a vector, so the following gives `TypeError: Gradient only defined for scalar-output functions. Output had shape: (4,).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid_grad(some_vector) # uncomment if you want to try "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use `sigmoid_grad` to compute the gradient of the *scalar* function sigmoid on a batch (vector) of scalars, we can use another function transformation: `jax.vmap`.\n",
    "\n",
    "What `jax.vmap` does, is it takes a function f, and it returns a version of f that works on a batch of inputs (this is a vectorized function, so the computations happen in parallel). Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{jax.vmap(sigmoid_grad)(some_vector)=}\")\n",
    "print(f\"{sigmoid_grad_manual(some_vector)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for many functions (such as sigmoid and the derivative of a sigmoid), it is easy to write a function that automatically gets vectorized when applied to higher-dimensional input. But when writing more complicated functions, `jax.vmap` allows us to write the function for inputs for which we easily understand what it should look like, and then automatically apply it correctly to higher dimensional inputs without worrying about this. \n",
    "\n",
    "### Exercise \n",
    "Use the documentation of [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) to create the following function:\n",
    "$$\n",
    "\\mathrm{target\\_function\\_1}(\\mathrm{value}, \\mathrm{weights}) = \\left(\\left(\\nabla_x \\mathrm{self\\_changing\\_layer\\_square}\\right)(\\mathrm{value}, \\mathrm{weights})[0],\\  \\mathrm{self\\_changing\\_layer}(\\mathrm{value},\\, \\mathrm{weights}) [1]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_changing_layer_square(x, weights):\n",
    "    out, new_weights = self_changing_layer(x, weights)\n",
    "    return jnp.square(out).sum(), new_weights\n",
    "\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the following code block, you are given an array cotaining 20 different $3\\times 3$ matrices, and an array containing 30 different $3$-vectors. Use `vmap` (and a hand written function for matrix-vector multiplication) to create a $20\\times 30\\times 3$ array with in location $(i, j)$ a $3$-vector contining the result of multiplying the $i^{\\text{th}}$ matrix with the $j^{\\text{th}}$ vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = jnp.linspace(0., 1., 180).reshape((20, 3, 3))\n",
    "vectors = jnp.linspace(0., 1., 90).reshape((30, 3))\n",
    "\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random numbers\n",
    "When we use 'random' numbers in machine learning, they typically aren't truely random. Instead, we are using a [pseudo random number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) (**PRNGs**) to create (deterministic) sequences of outputs that the right statistical properties. These PRNGs typically have a *seed*, which determines what sequence is going to be generated, and some internal state. In e.g. numpy and pytorch, you can set this seed, and the (underlying) PRNG that is used then automatically updates its state every time you draw a random number.\n",
    "\n",
    "Now, as we have discussed above, JAX doesn't play too nicely with global states. Instead, we have to keep track of the PRNG state, typically refered to as a **key**, ourselves. If we pass the same key to the same random function twice, we get the same result twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "key = random.key(123)\n",
    "print(key)\n",
    "\n",
    "print(f\"\\n{random.normal(key, shape=(3,))=}\")\n",
    "print(f\"{random.normal(key, shape=(3,))=}\\n\")\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get new pseudo random numbers, we need to split the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "print(key)\n",
    "print(subkey)\n",
    "print(f\"\\n{random.normal(subkey, shape=(3,))=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing this, **make sure not to re-use used keys!** A good habit is to only ever use subkey to pass to functions, and use key to keep track of your random state. So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "print(random.normal(subkey))\n",
    "key, subkey = random.split(key)\n",
    "print(random.normal(subkey))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that key and subkey are just `jax.Array`s, so we can e.g. `vmap` over subkey if need be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_complicated_random_function(key):\n",
    "    # imagine that this is some complicated code that is hard to manually write in a vectorized way\n",
    "    return random.normal(key)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "subkey_array = random.split(subkey, 5)  # if we wanted say a grid instead, we could instead do random.split(subkey, (5, 5)) where (5, 5) would be the shape of the grid of subkeys\n",
    "jax.vmap(my_complicated_random_function)(subkey_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Look at the documentation of [`jax.lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan) and [`jax.lax.select`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html#jax.lax.select) to write a function that performs the following random walk by scanning over an array of prng keys:\n",
    "\n",
    "$$\n",
    "x^0 = (0, 0)\\\\\n",
    "x^i = \\begin{cases}x^{i-1}+\\epsilon^i&\\text{with probability }\\frac{1}{2}\\\\(x^{i-1}_1, x^{i-1}_0) + \\epsilon^i&\\text{with probability }\\frac{1}{2}\\end{cases}\\\\\n",
    "\\text{where}\\\\\n",
    "\\epsilon^i \\sim \\mathcal{N}(\\underline{0}, I)\n",
    "\n",
    "$$\n",
    "\n",
    "Then use [`jax.vmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html) to simulate 10 random walks of length 20 in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side-note, although there are good reasons for JAX to take this approach to PRNGs, it can be a bit cumbersome to do this by hand all the time, especially when you're e.g. writing neural network architectures, where each layer will need a prng key for initialization and you might not know by heart beforehand how many keys you will need. I've written a package with convenience utilities called `common_jax_utils` that includes a generator `key_generator` that you can use in such situations.\n",
    "\n",
    "I would recommend against using it in any code that needs to be jitted or vmapped, although, as long as you write all of your functions in a way that they do not have side-effects (**so don't write functions that expect a key_generator as an argument**), you should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common_jax_utils as cju\n",
    "key, subkey = random.split(key)\n",
    "key_gen = cju.key_generator(subkey)\n",
    "\n",
    "print(f\"{random.normal(next(key_gen))=}\")\n",
    "print(f\"{random.normal(next(key_gen))=}\")\n",
    "print(f\"{random.normal(next(key_gen))=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controll flow and function transformations\n",
    "### jitting / vmapping\n",
    "Sometimes, we need to use control flow, such as `if`-`else` statements or `for` loops. This doesn't always play nice with `jax.jit` and `jax.vmap`. In some of the above exercises, we already saw some `jax.lax` control flow alternatives such as `jax.lax.select` and `jax.lax.scan`. The question then remains: when should we use these control flow functions from `jax.lax`, and when is it okay to just use python control statements?\n",
    "\n",
    "https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow provides a good, elaborate explanation of this, and you should read this. Nonetheless, I'd like to provide a quick rule of thumb with a rationale behind it here.\n",
    "\n",
    "Basically, `jax.jit` keeps track of the shape and dtype of arguments to a jitted function. When a combination of argument shapes/dtypes is encountered that hasn't been encountered before, the function gets compiled *for this specific combination* and the compiled code gets stored, so that when this combination is encountered again, the compiled code can be used again and no re-compilation is necessary. If you play around with this, you'll find that the first time you use a jitted function on a specific array shape, it takes *much* longer than each next time you use it on the same shape.\n",
    "\n",
    "This brings us to the **first rule of thumb** when it comes to control flow and jitting functions: *when the control flow depends **only on the shapes/dtypes** of arguments, you can safely use python control flow statements*. Some examples are:\n",
    "`if len(input_array.shape) == 3:...` or `if input_array.shape[3] % 2 == 0:...` or `if input_array.dtype == jnp.int16:...`.\n",
    "In principle the same holds for e.g. loops, but *if you use long loops (i.e. with many iterations) within a jitted function, the compilation step will likely take very long!*\n",
    "\n",
    "The first step towards jit-compiling a function is called tracing. The function is run on *tracer* objects in order to record the sequence of operations specified by the function. These tracer objects encode the shape and dtype of what they are representing, **but not the actual values**. That means that if you use regular Python control flow (like `if` and `else`, or `while`) based on the values, that you will either get errors, or wrong results. \n",
    "\n",
    "This brings us to the **second rule of thumb** when it comes to control flow and jitting functions: *when the control flow depends on the **values** of arguments, you should use [control flow primitives](https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators).* For example `jax.lax.select(r<.5, x_i, x_i[::-1])`. \n",
    "\n",
    "\n",
    "Another useful resource on this, that you should definitely read, is https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n",
    "\n",
    "\n",
    "As a final **TL;DR on jitting and control flow**:\n",
    "* values depending on values **requires control flow operators**\n",
    "* values depending on shapes is fine (you can use python control flow statements)\n",
    "* shapes depending on shapes is fine, (as long as you don't use jax to make intermediate computations, so `jnp.zeros((np.prod(x.shape),))` is fine but `jnp.zeros((jnp.prod(x.shape)))` wil cause problems when jitted)\n",
    "* shapes depending on values is **not fine** (although in some cases you can get around this by indicating to `jax.jit` that some arguments should not be traced by marking them as *static*).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example making tracing explicit\n",
    "@jax.jit  # decorator syntax, see e.g https://pythonbasics.org/decorators/\n",
    "def f(x, y):\n",
    "    print(\"Running f():\")\n",
    "    print(f\"  x = {x}\")\n",
    "    print(f\"  y = {y}\")\n",
    "    result = jnp.dot(x + 1, y + 1)\n",
    "    print(f\"  result = {result}\")\n",
    "    return result\n",
    "\n",
    "x = np.random.randn(3, 4)\n",
    "y = np.random.randn(4)\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a side-note, if you want to print values for e.g. debugging purposes, just using the Python `print` function will not give you the desired result when working with jitted functions. Read the material in https://jax.readthedocs.io/en/latest/debugging/index.html and in https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html for more on the topic of **debugging transformed functions**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Complete the following python functions in a way that they can be correctly jit-compiled and then test the jit-compiled versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f1(x:jax.Array):\n",
    "    \"\"\" \n",
    "    :parameter x: jax.Array (of any shape)\n",
    "    :return: an array with elements y_i = x_i**2 if x_i <0 else x_i **3\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    pass\n",
    "\n",
    "def f2(a: jax.Array, b: jax.Array):\n",
    "    \"\"\" \n",
    "    :parameter a: jax.Array of shape (n_0, ..., n_k)\n",
    "    :parameter b: jax.Array of shape (m_0, ..., m_k)\n",
    "    :return: a jax.Array c of shape (max(n_0, m_0), ..., max(n_k, m_k))\n",
    "        with c[i_0..., i_k] = aa[i_0, ..., i_k] + bb[i_0, ..., i_k]\n",
    "        where aa[i_0, ..., i_k] = a[i_0, ..., i_k] if i_0<n_0, ..., i_k < n_k else max(n_0, ..., n_k)\n",
    "        and b[i_0, ..., i_k] = b[i_0, ..., i_k] if i_0<m_0, ..., i_k < m_k else max(m_0, ..., m_k)\n",
    "    \"\"\"\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control flow and grad\n",
    "Mostly there is no problem here (until we want to also `jit` the `grad` of a function). When using `lax.while_loop` or `lax.fori_loop`, automatic differentiation is restricted to [forward mode differentiation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobian-vector-products-jvps-aka-forward-mode-autodiff) (see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#summary). But you're unlikely to run into this during this course.\n",
    "\n",
    "One salient side-note about control-flow and automatic differentiation, is that if you have a function that is only partly defined, such as `jnp.log`, and you want to make it safe using `jnp.where`, you should put the `jnp.where` **inside** the `jnp.log` so as to prevent *NaN*s from occuring: see https://jax.readthedocs.io/en/latest/faq.html#gradients-contain-nan-where-using-where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytrees and related utilities\n",
    "Before we really get into defining and training Neural Networks in JAX, there is one more very important concept in JAX: pytrees. Before we get to what they really are, you can informally think of pytrees as any container (of containers of...) of `jax.Array`s. This really isn't quite accurate, but for now it's good enough.\n",
    "\n",
    "Why are these relevant? Well, remember that because our functions have to be pure, any state that we might want to have needs to be passed around between them? That state will often not be just at single array. Think of e.g. all the weights and biases of a Neural Network, and think of the momentum for all those weights and biases that some optimizers need to keep track of. For an MLP it might be reasonable to just put all of them into a tuple, but when we work with more complicated networks, it might be helpful for the structure in which we store all of these weights and biases to resemble the architecture of the network.\n",
    "\n",
    "Fortunately, many jax functions, such as `jax.jit`, `jax.grad`, `jax.vmap`, and `jax.lax.scan` play really nicely with these pytrees. And for functions that don't, there are utilities such as `jax.tree.map` and `jax.tree_util.tree_map_with_path`. Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_jax_utils.debug_utils import summary\n",
    "\n",
    "def some_function(a:jax.Array, b:tuple[jax.Array, jax.Array]):\n",
    "    \"\"\" \n",
    "    :parameter a: array of shape (in_channels,)\n",
    "    :parameter b: tuple of two arrays\n",
    "        first array of shape (out_channels, in_channels)\n",
    "        second array of shape (out_channels,)\n",
    "    :return: array of shape (out_channels,)\n",
    "    \"\"\"\n",
    "    return (b[0]@a)/(1+jnp.abs(b[1]))\n",
    "\n",
    "a = random.normal(next(key_gen), (10, 3))\n",
    "b = [\n",
    "    random.normal(next(key_gen), (2, 3)),\n",
    "    random.normal(next(key_gen), (2,))\n",
    "]\n",
    "\n",
    "print(f\"{summary(jax.vmap(some_function, in_axes=(0, None))(a, b))=}\") # vmap over the 0-axis of a and over none of the axes in any element in b\n",
    "\n",
    "b_alt = (\n",
    "    random.normal(next(key_gen), (2, 3)),\n",
    "    random.normal(next(key_gen), (2, 10))\n",
    ")\n",
    "\n",
    "print(f\"{summary(jax.vmap(some_function, in_axes=(0, (None, 1)))(a, b_alt))=}\") # vmap over the 0-axis of a and over none of the axes in the first element of b_alt and over the 1-axis of the second element of b_alt\n",
    "\n",
    "def another_function(a:jax.Array, b:tuple[jax.Array, jax.Array]):\n",
    "    \"\"\" \n",
    "    :parameter a: array of shape (batch, in_channels)\n",
    "    :parameter b: tuple of two arrays\n",
    "        first array of shape (out_channels, in_channels)\n",
    "        second array of shape (out_channels,)\n",
    "    :return: array of shape (out_channels,)\n",
    "    \"\"\"\n",
    "    return jnp.mean(\n",
    "        jnp.square(\n",
    "            jax.vmap(some_function, in_axes=(0, None))(a, b)\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"{summary(jax.value_and_grad(another_function, argnums=1)(a, b))=}\")  # value and gradient w.r.t b, where gradient has the same shape as b, including the same pytree structure\n",
    "summary(\n",
    "    jax.vmap(jax.value_and_grad(another_function, argnums=1), in_axes=(None, (None, 1)))(a, b_alt)  # can you explain why every array in the result has 10 for its 0-axis?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "Complete the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at these concepts in action by writing a simple implementation of an mlp and its training loop\n",
    "from typing import Sequence, Union  # for Sequence: think list or tuple\n",
    "\n",
    "def simple_mlp(x:jax.Array, weights:Union[Sequence[jax.Array], jax.Array], biases:Union[Sequence[jax.Array], jax.Array]):\n",
    "    \"\"\"simple_mlp \n",
    "    Apply an MLP with weights and biases dicated by the weights and biases parameters, and ReLU activations, to the datapoint x.\n",
    "    The final layer of the MLP is a linear one.\n",
    "\n",
    "    :parameter x: jax.Array of shape (in_channels,)\n",
    "    :parameter weights: Either a jax.Array of shape (out_channels, in_channels), \n",
    "        or a Sequence of jax.Arrays, the first one of shape (hidden_channels_0, in_channels) \n",
    "        and the last one of shape (out_channels, hidden_channels_n)\n",
    "    :parameter biases: Either a jax.Array of shape (out_channels,),\n",
    "        or a Sequence of jax.Arrays, the first one of shape (hidden_channels_0,)\n",
    "        and the last one of shape (out_channels,)\n",
    "    :raises ValueError: in case the number of weights arrays isn't equal to the numer of biases arrays\n",
    "    :return: MLP applied to x\n",
    "    \"\"\"\n",
    "    # first do some case handling for the different input types (jax.Array vs Sequence[jax.Array])\n",
    "\n",
    "    # then check if we need to raise a ValueError\n",
    "\n",
    "    # finally implement the logic of the MLP\n",
    "    # hint for the ReLU activations you can use jax.nn.relu\n",
    "    pass\n",
    "\n",
    "\n",
    "weights = (random.normal(next(key_gen), (16, 3)), random.normal(next(key_gen), (16, 16)), random.normal(next(key_gen), (1, 16)))  # NB this is really bad initialization\n",
    "biases = (random.normal(next(key_gen), (16,)), random.normal(next(key_gen), (16,)), random.normal(next(key_gen), (1,)))\n",
    "batch = random.normal(next(key_gen), (10, 3))\n",
    "\n",
    "# uncomment to test your code\n",
    "# print(f\"{cju.debug_utils.summary(jax.vmap(simple_mlp, in_axes=(0, None, None))(batch, weights, biases))=}\")  # apply simple_mlp to a batch of inputs\n",
    "\n",
    "def calculate_loss(xs:jax.Array, ys:jax.Array, weights:tuple[jax.Array, ...], biases:tuple[jax.Array, ...]):\n",
    "    \"\"\"\n",
    "    apply simple_mlp on xs and compute the mean square error\n",
    "    :parameter xs: jax.Array with input data\n",
    "    :parameter ys: target values to be approximated by the mlp\n",
    "    :parameter weights: weigths of the MLP\n",
    "    :parameter biases: biases of the MLP\n",
    "    :return: scalar representing the mean squared error\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def simple_train_step(\n",
    "        weights_and_biases: tuple[Sequence[jax.Array], Sequence[jax.Array]], \n",
    "        data: tuple[jax.Array, jax.Array]\n",
    "        ) -> tuple[tuple[Sequence[jax.Array], Sequence[jax.Array]], jax.Array]:\n",
    "    \"\"\" \n",
    "    Perform a train step with a fixed learning rate of .01\n",
    "    The signature of this function (what kind of arguments it takes and what it returns) makes it so that it can be used with jax.lax.scan\n",
    "    :parameter weights_and_biases: 2-tuple with as its first element the weights of an MLP and as its second element the biases of an MLP (both compatible with)\n",
    "    :parameter data: 2-tuple with as its first element a jax.Array representing a batch of input data, and as its second element a jax.Array containing the corresponding labels\n",
    "    :return: 2-tuple, the first element of which contains a new value for weights_and_biases, and the second element of which contains the computed loss\n",
    "    \"\"\"\n",
    "    xs, ys = data\n",
    "    weights, biases = weights_and_biases\n",
    "    # hint: use jax.value_and_grad to get the loss and gradients, and use jax.tree.map for getting the updated weights and biases\n",
    "    \n",
    "    #return (new_weights, new_biases), loss\n",
    "\n",
    "ys = random.normal(next(key_gen), (10, 1))\n",
    "\n",
    "training_data = random.normal(next(key_gen), shape=(20, 100, 3))  # 20 batches of size 100\n",
    "training_labels = jnp.sin(training_data)\n",
    "\n",
    "# uncomment the following to test your implementation\n",
    "# print(f\"{cju.debug_utils.summary(simple_train_step((weights, biases), (batch, ys)))=}\")\n",
    "\n",
    "# final_weights_and_biases, losses = jax.lax.scan(simple_train_step, init=(weights, biases), xs=(training_data, training_labels))\n",
    "# print(cju.debug_utils.summary(final_weights_and_biases))\n",
    "# print(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, thinking of Pytrees just as compositions of Python containers and `jax.Array`s isn't quite right. The containers needn't be just standard Python containers, and the leafs of the tree needn't just be `jax.Array`s. Instead, what counts as a container for a pytree is any object whose type is registered in JAX's pytree container registry. And any object whose type is not in that registry is considered a leaf of a pytree. So, the definition given by the [JAX documentation](https://jax.readthedocs.io/en/latest/pytrees.html#what-is-a-pytree) is:\n",
    "1. any object whose type is *not* in the pytree container registry is considered a *leaf* pytree;\n",
    "1. any object whose type is in the pytree container registry, and which contains pytrees, is considered a pytree.\n",
    "\n",
    "[If you want, you can write and register your own pytree container types](https://jax.readthedocs.io/en/latest/pytrees.html#extending-pytrees). But likely you won't need to, as any subclass of `equinox.Module`, our next topic of interest, is automatically registered as a pytree container. Another source of pytrees that you will encounter is `optax` with its optimizer states and gradient updates.\n",
    "\n",
    "Utilities for working with pytrees can be found in [`jax.tree`](https://jax.readthedocs.io/en/latest/jax.tree.html), [`jax.tree_util`](https://jax.readthedocs.io/en/latest/jax.tree_util.html#module-jax.tree_util), [`common_jax_utils.tree_utils`](https://github.com/SimonKoop/common_jax_utils/blob/main/src/common_jax_utils/tree_utils.py), [`optax.tree_utils`](https://optax.readthedocs.io/en/latest/api/utilities.html#tree), and [`equinox`](https://docs.kidger.site/equinox/api/manipulation/).\n",
    "\n",
    "Equinox is particularly useful when some of the leafs in your pytree are not actually `jax.Array`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Equinox](https://docs.kidger.site/equinox/)\n",
    "\n",
    "Unlinke Pytorch and Tensorflow, JAX itself isn't a deeplearning library. There are various Neural Network libraries built ontop of JAX such as Flas, Haiku, and Equinox. In this course, we'll be using Equinox because it is conceptually simple (and frankly because I had a bunch of code for an old research project lying around that can make things significantly easier for you, and that code was written using Equinox). Importantly, Equinox makes it easy to do object oriented programming very similarly to Pytorch, but resulting in pytrees, so that the result is compatible with anything else in the JAX ecosystem.\n",
    "\n",
    "## Equinox Modules\n",
    "Writing (neural network) models in Equinox is done by subclassing [`equinox.Module`](https://docs.kidger.site/equinox/api/module/module/). The resulting code looks quite similar to Pytorch code (where writing models happens through subclassing `torch.nn.Module`), but there are some key differences:\n",
    "* `equinox.Module`s are **static** (remember, to jit things, our functions need to be pure, i.e. without side-effects)\n",
    "* `equinox.Module`s are pytrees (they are registered as pytree containers automatically)\n",
    "* `equinox.Module`s are [*dataclasses*](https://docs.python.org/3/library/dataclasses.html) and all of their attributes need to be registered as *fields*. This also means they come with a default `__init__` method.\n",
    "* `equinox.Module`s will typically require a PRNG key for initializing weights and biases with random numbers\n",
    "* the forward pass of your model can be defined in its `__call__` method (or you can just use any other method you want).\n",
    "\n",
    "One particularly nice property of `equinox.Module`s, is that because they are pytrees, and because everything in JAX plays well with pytrees, you can have an `equinox.Module` that returns another `equinox.Module` in its forward pass, and everything will just work nicely the way you would want it to. We'll see this in the next practical session where we get more accustomed to the course code-base.\n",
    "\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "from typing import Optional\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# let's first re-define simple_mlp from above as a eqx.Module\n",
    "# eqx.nn actually provides an MLP class, and moreover, it provides layers such as nn.Linear that you should ideally use for defining neural networks\n",
    "# but this does provide a good, very basic example for now\n",
    "class SimpleMLP(eqx.Module):\n",
    "    weights: Sequence[jax.Array]  # we need to register the attributes as fields\n",
    "    biases: Sequence[jax.Array]  # their values can only be set during initialization of the model\n",
    "\n",
    "    def __init__(self, in_size:int, out_size:int, hidden_size:Union[int, Sequence[int]], key:jax.Array, depth:Optional[int]=None):\n",
    "        \"\"\" \n",
    "        :parameter in_size: number of input features\n",
    "        :parameter out_size: number of output features\n",
    "        :parameter hidden_size: either an integer or a sequence of integers, the number of hidden units in each layer\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        :parameter depth: number of hidden layers (ignored if hidden_size is a sequence of integers)\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        if isinstance(hidden_size, int) and depth is not None:\n",
    "            hidden_size = depth*(hidden_size,)\n",
    "        elif isinstance(hidden_size, int):  # depth is None\n",
    "            raise ValueError(\"If hidden size is an integer, depth should be provided do determine the number of hidden layers\")\n",
    "        elif depth is not None: # hidden size is not an integer\n",
    "            warnings.warn(f\"The value provided for depth ({depth}) will be ignored because hidden_size is provided as a sequence ({hidden_size})\")\n",
    "\n",
    "        output_sizes = tuple(hidden_size) + (out_size,)\n",
    "        input_sizes = (in_size, ) + tuple(hidden_size)\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for out_size, in_size in zip(output_sizes, input_sizes):\n",
    "            lim = 1/math.sqrt(in_size)\n",
    "            self.weights.append(random.uniform(\n",
    "                next(key_gen), \n",
    "                (out_size, in_size), \n",
    "                minval=-lim,\n",
    "                maxval=lim\n",
    "                ))\n",
    "            self.biases.append(random.uniform(\n",
    "                next(key_gen),\n",
    "                (out_size,),\n",
    "                minval=-lim,\n",
    "                maxval=lim\n",
    "            ))\n",
    "    \n",
    "    def __call__(self, x:jax.Array)->jax.Array:\n",
    "        \"\"\" \n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (in_size,)\n",
    "        :return: output of the MLP, jax.Array of shape (out_size,)\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            h = jax.nn.relu(w @ h + b)\n",
    "        w, b = self.weights[-1], self.biases[-1]\n",
    "        return w @ h + b\n",
    "    \n",
    "example_mlp = SimpleMLP(3, 1, (16, 17, 16), key=next(key_gen))\n",
    "print(example_mlp)\n",
    "\n",
    "x = random.normal(next(key_gen), (100, 3))\n",
    "print(f\"{summary(jax.vmap(example_mlp)(x))=}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For another example, let's now write a model that actually uses the compositional nature of `eqx.Module`s and uses some of the pre-defined layers from `eqx.nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCResBlock(eqx.Module):\n",
    "    norm_layer_0: eqx.Module\n",
    "    norm_layer_1: eqx.Module\n",
    "    linear_layer_0: eqx.Module\n",
    "    linear_layer_1: eqx.Module\n",
    "\n",
    "    def __init__(self, feature_size, key):\n",
    "        \"\"\"\n",
    "        :parameter feature_size: number of features\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        self.norm_layer_0 = eqx.nn.LayerNorm(  # we are using LayerNorm out of convenience here\n",
    "            shape=(feature_size,)\n",
    "        )\n",
    "        self.norm_layer_1 = eqx.nn.LayerNorm(  # it's possible to use BatchNorm, but due to the stateful nature of BatchNorm, that would make this example slightly more involved\n",
    "            shape=(feature_size,)\n",
    "        )\n",
    "        self.linear_layer_0 = eqx.nn.Linear(\n",
    "            feature_size, feature_size,\n",
    "            key=next(key_gen)\n",
    "        )\n",
    "        self.linear_layer_1 = eqx.nn.Linear(\n",
    "            feature_size, feature_size,\n",
    "            key=next(key_gen)\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x:jax.Array):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (feature_size,)\n",
    "        :return: output of the residual block, jax.Array of shape (feature_size,)\n",
    "        \"\"\"\n",
    "        h = self.norm_layer_0(x)\n",
    "        h = jax.nn.relu(h)\n",
    "        h = self.linear_layer_0(h)\n",
    "        h = self.norm_layer_1(h)\n",
    "        h = jax.nn.relu(h)\n",
    "        h = self.linear_layer_1(h)\n",
    "        return x + h\n",
    "    \n",
    "\n",
    "class FCResnet(eqx.Module):\n",
    "    input_layer: eqx.nn.Linear\n",
    "    res_blocks: list[FCResBlock]\n",
    "    output_layer: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, in_features:int, out_features:int, hidden_features:int, num_resblocks:int, key:jax.Array):\n",
    "        \"\"\"\n",
    "        :parameter in_features: number of input features\n",
    "        :parameter out_features: number of output features\n",
    "        :parameter hidden_features: number of features in the hidden layers\n",
    "        :parameter num_resblocks: number of residual blocks\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        \"\"\"\n",
    "        key_gen = cju.key_generator(key)\n",
    "        self.input_layer = eqx.nn.Linear(in_features, hidden_features, key=next(key_gen))\n",
    "        self.res_blocks = [\n",
    "            FCResBlock(hidden_features, key=next(key_gen))\n",
    "            for _ in range(num_resblocks)\n",
    "        ]\n",
    "        self.output_layer = eqx.nn.Linear(hidden_features, out_features, key=next(key_gen))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (in_features,)\n",
    "        :return: output of the MLP, jax.Array of shape (out_features,)\n",
    "        \"\"\"\n",
    "        h = self.input_layer(x)\n",
    "        for block in self.res_blocks:\n",
    "            h = block(h)\n",
    "        return self.output_layer(h)\n",
    "    \n",
    "my_resnet = FCResnet(3, 1, 32, 2, key=next(key_gen))\n",
    "print(my_resnet)\n",
    "print(f\"{summary(jax.vmap(my_resnet)(x))=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Write a ResNet class that uses [convolutional blocks](https://docs.kidger.site/equinox/api/nn/conv/#equinox.nn.Conv) and [batch normalization](https://docs.kidger.site/equinox/api/nn/normalisation/#equinox.nn.BatchNorm). Test it on some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered transformations\n",
    "When the only leafs of our `eqx.Module`s are `jax.Array`s, we can use JAX's built in function transformations like `vmap`, `grad`, and `jit` without any problems, because, as we saw before, those play nice with pytrees. You can see this in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(network, x, y):\n",
    "    return jnp.mean(jnp.square(jax.vmap(network)(x)-y))\n",
    "\n",
    "jitted_value_and_grad_loss = jax.jit(jax.value_and_grad(loss))\n",
    "jitted_value_and_grad_loss(example_mlp, x, random.normal(next(key_gen), (100, 1)))  # note that the grad has the same structure as the network, so it's a SimpleMLP itself!\n",
    "# (albeit not one that we would want to apply to data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for many models, we might want to store more information in the model than just arrays containing weights and biases. E.G. maybe we want to have some freedom in which activation function is used, and we want to store it as an attribute. This can cause some problems if we just use `jax.grad` or `jax.jit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class NNLayer(eqx.Module):\n",
    "    weights: jax.Array\n",
    "    biases: jax.Array\n",
    "    activation_function: Callable\n",
    "\n",
    "    def __init__(self, in_features:int, out_features:int, key:jax.Array, activation_function:Callable=jax.nn.relu):\n",
    "        \"\"\"\n",
    "        :parameter in_features: number of input features\n",
    "        :parameter out_features: number of output features\n",
    "        :parameter key: jax.Array, random key for initialization\n",
    "        :parameter activation_function: activation function to be used\n",
    "        \"\"\"\n",
    "        lim = 1/math.sqrt(in_features)\n",
    "        self.weights = random.uniform(\n",
    "            key, \n",
    "            (out_features, in_features), \n",
    "            minval=-lim,\n",
    "            maxval=lim\n",
    "            )\n",
    "        self.biases = random.uniform(\n",
    "            key,\n",
    "            (out_features,),\n",
    "            minval=-lim,\n",
    "            maxval=lim\n",
    "        )\n",
    "        self.activation_function = activation_function\n",
    "    \n",
    "    def __call__(self, x:jax.Array):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :parameter x: input data, jax.Array of shape (in_features,)\n",
    "        :return: output of the layer, jax.Array of shape (out_features,)\n",
    "        \"\"\"\n",
    "        return self.activation_function(self.weights@x+self.biases)\n",
    "\n",
    "my_layer = NNLayer(3, 1, key=next(key_gen))\n",
    "# uncomment the following line to try it out and get the TypeError:\n",
    "# jitted_value_and_grad_loss(my_layer, x, random.normal(next(key_gen), (100, 1)))  # TypeError: Cannot interpret value of type <class 'jax._src.custom_derivatives.custom_jvp'> as an abstract array; it does not have a dtype attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to deal with this, would be to manually indicate whicharguments should be treated statically. Doing so would however be rather inconvenient. Fortunately, equinox provides tools to deal with this instead: [filtered versions of many jax transformations](https://docs.kidger.site/equinox/api/transformations/). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jvag_loss = eqx.filter_jit(eqx.filter_value_and_grad(loss))\n",
    "filtered_jvag_loss(my_layer, x, random.normal(next(key_gen), (100, 1)))  \n",
    "# question: my_layer works on 3-vectors, but x has shape (100, 3), why shouldn't we put jax.vmap(my_layer) instead of my_layer in filtered_jvag_loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optax](https://optax.readthedocs.io/en/latest/index.html)\n",
    "Earlier in this notebook, we trained a neural network (our `simple_mlp`) by applyin a gradient update sort of manually using `jax.tree.map`:\n",
    "```python\n",
    "new_weights, new_biases = jax.tree.map(\n",
    "    lambda w, g: w-.01*g,  # gradient descent with learning rate 0.01\n",
    "    (weights, biases),\n",
    "    grads\n",
    ")\n",
    "```\n",
    "In general however, we might want to modify the gradients slightly, e.g. by clipping them, or we might want to use optimization techniques such as momentum. \n",
    "\n",
    "In Pytorch, we have `Optimizer` objects that track state like momentum, and apply the correct updates to the model. In JAX however, we want operations to have no side-effects so that we can JIT them. That means we'll have to keep track of any state (such as momentum) explicitly, and we will have to combine our model with any updates into a new model since the model can't be changed (as that would be a side-effect).\n",
    "\n",
    "Nonetheless, we don't have to do all of the work of transforming the gradients in combination with a state into updates to our network and a new state manually every time. Optax is an optimization library for JAX that implements many commonly used optimizers, such as the Adam optimizer.\n",
    "\n",
    "The first core concept of Optax is that of the `GradientTransformation`. A `GradientTransformation` object contains an `update` method that takes gradients, a state, and (optionally) the current parameters that are to be updated, and transforms them into updates for the model and a new state. One example of such a transformation is gradient clipping ([`optax.clip`](https://optax.readthedocs.io/en/latest/api/transformations.html#optax.clip)): it takes a pytree of gradients and a state (and optionally the parameters), and returns a clipped version of the pytree of gradients and the same state.\n",
    "\n",
    "Another example is ['optax.adam'](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.adam): the resulting `GradientTransformation` takes a pytree of gradients, an *appropriate* state (a.o. containing a $1^{\\text{st}}$ moment pytree and a $2^{\\text{nd}}$ moment pytree), and optionally a pytree of parameters, and computes new moments for the state and returns updates based on those new moments and the new state. \n",
    "\n",
    "These gradient transformations are composable, so that we can combine the transformation from `optax.clip` with that from `optax.adam` into a `GradientTransformation` that first clips the gradients and then applies the updaterules from Adam.\n",
    "\n",
    "Moreover, they don't only implement their update rule (in their `update` method), but they also implement an `init` method that takes a pytree of model parameters (e.g. an `eqx.Module`) and returns an appropriate optimizer state (an instance of `OptState`, which makes it a pytree as well).\n",
    "\n",
    "**In summary**, to use optimizers from Optax, what we need to do is\n",
    "1. define a model, say `model` (e.g. an `eqx.Module`)\n",
    "1. create a `GradientTransform`, say `optimizer` (e.g. by calling `optax.adam(learning_rate=.01)`)\n",
    "1. create an appropriate `OptState` by setting `opt_state = optimizer.init(model)`\n",
    "\n",
    "and then during our training loop, at each step we\n",
    "1. compute the gradients of the loss (and the loss itself) using `jax.value_and_grad` or `eqx.filter_value_and_grad`\n",
    "1. we combine the gradients and the existing optimizer state into updates and a new optimizer state (think `updates, opt_state = optimizer.update(grads, opt_state, model)`)\n",
    "1. we combine our existing model and the updates into a new model\n",
    "\n",
    "This last step can be done either by using `optax.apply_updates` or [`eqx.apply_updates`](https://docs.kidger.site/equinox/api/manipulation/#equinox.apply_updates) (or by using `jax.tree.map` to apply the updates manually). When working with `eqx.Module`s, it's typically best to use `eqx.apply_updates` for basically the same reason as why we want to use `eqx.filter_grad` and `eqx.filter_jit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train example_mlp from earlier \n",
    "import optax \n",
    "\n",
    "\n",
    "optimizer = optax.adam(.01)\n",
    "initial_state = optimizer.init(eqx.filter(example_mlp, eqx.is_array))\n",
    "# the filter isn't really necessary for example_mlp, but it is for any model that contains\n",
    "# non-array attributes, such as the NNLayer class above\n",
    "\n",
    "def loss_func(model, xs, ys):\n",
    "    predictions = jax.vmap(model)(xs)\n",
    "    return jnp.mean(jnp.square(predictions-ys))\n",
    "\n",
    "#@eqx.filter_jit  # see text after this cell\n",
    "def train_step(model_and_state, data):\n",
    "    model, opt_state = model_and_state\n",
    "    xs, ys = data\n",
    "    loss, grads = eqx.filter_value_and_grad(loss_func)(model, xs, ys)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state, model)\n",
    "    new_model = eqx.apply_updates(model, updates)\n",
    "    return (new_model, new_opt_state), loss\n",
    "\n",
    "\n",
    "# now we train the model\n",
    "example_mlp_trained, losses = jax.lax.scan(train_step, init=(example_mlp, initial_state), xs=(training_data, training_labels))\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note about the commented-out `@eqx.filter_jit` above the definition of `train_step` in the above example: in this example we use `jax.lax.scan` for the training loop. As mentioned in the \"Note\" in the [documentation of `jax.lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan), `scan()` compiles the function that is scanned, so it is unnecessary to manually jit `train_step` in this case.\n",
    "\n",
    "However, in practice, we often want to do more than just train a model during our training loop: we might want to give live updates about the loss and about various metrics to [\"weights and biases\" (wandb)](https://wandb.ai), and we might want to run a validation loop after every `N` steps, and maybe store some intermediate results, etc. This is much easier when we just run a Python `for` loop, calling the `train_step` function at every iteration. \n",
    "\n",
    "You may wonder: do we lose anything by running a Python `for` loop instead of using `jax.lax.scan`? Afterall, Python loops are slow, right? In all honesty, I don't know for sure. You may want to experiment with this. If I had to give an educated guess however, I would say: as long as your model is large enough, you shouldn't miss anything. Or more precisely: as long as a single train step takes longer for the GPU to compute than that the python interpreter takes to interpret the (hopefully hand full of) lines in the training loop, [asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#async-dispatch) should mean that there is no real overhead of the python interpreter.\n",
    "\n",
    "So then in the usual case, where your train loop is just a Python `for` loop, **what functions should you jit?** Basically, only the `train_step` (and maybe e.g. a `validation_step` if you have implemented it). *As a rule of thumb, you want to jit the largest possible function for the best result.* So **don't** jit your model and your loss function etc. separately, passing a jitted model to the jitted `value_and_grad` of a jitted loss function. But instead **only jit the function that combines all of them** (i.e. the `train_step`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Define a model using `equinox.nn.sequential` and using the `NNLayer`s defined earlier in this notebook. Define a `train_step` function, and train your model using [`optax.amsgrad`](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.amsgrad) for 20 000 steps on the data provided in the next cell, and print the loss every 400 steps. After every 2 000 steps, compute the loss of your model on the validation data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = random.normal(next(key_gen), shape=(40000, 3))\n",
    "training_labels = jnp.sin(training_data[0]) - jnp.cos(training_data[1]) + jnp.sin(training_data[2] - 0.1*training_data[0] + 0.1*training_data[1])\n",
    "training_labels = training_labels[:, None]\n",
    "\n",
    "validation_data = random.normal(next(key_gen), shape=(2000, 3))\n",
    "validation_labels = jnp.sin(validation_data[0]) - jnp.cos(validation_data[1]) + jnp.sin(validation_data[2] - 0.1*validation_data[0] + 0.1*validation_data[1])\n",
    "validation_labels = validation_labels[:, None]\n",
    "\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of additional resources\n",
    "Here is a (far from comprehensive) list of additional material to look at if you get stuck with anything JAX related:\n",
    "* https://docs.kidger.site/equinox/faq/  answers to questions you'll likely at some point have (e.g. \"How to mark arrays as non-trainable?\")\n",
    "* https://docs.kidger.site/equinox/api/serialisation/  for storing your Equinox models\n",
    "* https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html for more automatic differentiation than just `jax.grad`\n",
    "* https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html for information on problems you're likely to run into\n",
    "* https://jax.readthedocs.io/en/latest/faq.html for more answers to questions you might have at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
